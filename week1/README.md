# Semana 1 - LLM Zoomcamp

Este documento recopila mis apuntes y recursos para la **Semana 1** del curso LLM Zoomcamp.

## ðŸ“ Notas de la teorÃ­a

### Large Language Model (LLM)

Un LLM (Large Language Model) es un modelo de lenguaje basado en redes neuronales profundas, entrenado para predecir el siguiente token (una palabra, parte de una palabra o sÃ­mbolo) en una secuencia de texto. A partir de grandes volÃºmenes de datos textuales, aprende patrones sintÃ¡cticos, semÃ¡nticos y contextuales, lo que le permite generar respuestas coherentes, relevantes y con apariencia humana.

Los LLM mÃ¡s avanzados estÃ¡n compuestos por miles de millones o incluso billones de parÃ¡metros. Estos parÃ¡metros â€”los pesos de la red neuronalâ€” representan el conocimiento adquirido durante el entrenamiento.

El rendimiento de un LLM depende en gran medida de tres factores:
* La cantidad y diversidad del corpus de entrenamiento
* La escala del modelo (nÃºmero de parÃ¡metros)
* Y su arquitectura, siendo el Transformer la base de los modelos modernos.

**Â¿CÃ³mo predice tokens un LLM?**
1. Se introduce un prompt o secuencia inicial de texto.
2. El modelo calcula la probabilidad de cada posible token que podrÃ­a continuar la secuencia.
3. Se selecciona el token mÃ¡s probable (o uno entre los mÃ¡s probables, usando tÃ©cnicas como muestreo o top-k).
4. El token elegido se agrega al texto y el proceso se repite hasta completar la respuesta.

### Retrieval-Augmented Generation (RAG)

En el Ã¡mbito de los Modelos de Lenguaje Grandes (LLMs), una de las limitaciones recurrentes ha sido su tendencia a "alucinar" o generar informaciÃ³n incorrecta o no verificable. Aunque los LLMs son increÃ­blemente potentes para comprender y generar texto, su conocimiento se limita a los datos con los que fueron entrenados. AquÃ­ es donde entra en juego **RAG (Retrieval-Augmented Generation)**, una tÃ©cnica que busca solucionar esta deficiencia, permitiendo a los LLMs acceder a fuentes de informaciÃ³n externas y verificables en tiempo real, mejorando significativamente la precisiÃ³n y relevancia de sus respuestas.

La idea central de RAG es combinar la capacidad generativa de un LLM con la capacidad de recuperaciÃ³n de informaciÃ³n de un sistema de bÃºsqueda. En lugar de generar una respuesta basÃ¡ndose Ãºnicamente en su entrenamiento interno, un modelo RAG primero busca y recupera documentos o pasajes relevantes de una base de datos externa, que puede ser tan vasta como se desee (documentaciÃ³n interna de una empresa, una base de conocimientos, artÃ­culos cientÃ­ficos, etc.).

El proceso se desarrolla en dos fases principales. Primero, cuando un usuario plantea una pregunta o una consulta, esta se utiliza para buscar informaciÃ³n en la base de datos externa. Esta bÃºsqueda se realiza tÃ­picamente utilizando tÃ©cnicas de incrustaciÃ³n (embeddings) que transforman el texto de la consulta y los documentos en vectores numÃ©ricos, permitiendo medir la similitud semÃ¡ntica. Los documentos mÃ¡s relevantes se recuperan para la siguiente etapa.

En la segunda fase, la informaciÃ³n recuperada, junto con la consulta original del usuario, se alimenta al LLM. El LLM entonces utiliza este contexto adicional, que es fidedigno y especÃ­fico, para generar su respuesta. Este enfoque permite que el modelo genere respuestas mÃ¡s precisas, fundamentadas en hechos y adaptadas a la informaciÃ³n mÃ¡s reciente o especÃ­fica que no estaba presente en sus datos de entrenamiento iniciales.

La principal ventaja de RAG es que reduce drÃ¡sticamente las alucinaciones y permite que los LLMs respondan preguntas sobre informaciÃ³n muy especÃ­fica o datos que no existÃ­an en el momento de su entrenamiento. Es una soluciÃ³n ideal para aplicaciones que requieren alta precisiÃ³n y verificabilidad, como chatbots corporativos, asistentes de soporte tÃ©cnico o sistemas de preguntas y respuestas basados en bases de conocimientos. AdemÃ¡s, al no requerir un reentrenamiento completo del LLM cada vez que se actualiza la base de conocimientos, RAG ofrece una soluciÃ³n mÃ¡s eficiente y escalable para mantener los LLMs actualizados y relevantes.

En resumen, RAG transforma los LLMs de ser meros generadores de texto basados en su memoria de entrenamiento a convertirse en potentes "investigadores" capaces de consultar y sintetizar informaciÃ³n externa, brindando respuestas mÃ¡s ricas, precisas y confiables. Es un paso crucial hacia LLMs verdaderamente informados y Ãºtiles en una amplia variedad de aplicaciones.

![alt text](./img/image.png)

## ðŸ› ï¸ Ejemplo prÃ¡ctico de RAG (Retrieval-Augmented Generation

### CreaciÃ³n de un entorno de desarrollo

Para crear y gestionar el entorno de Python de este proyecto se utiliza `uv`, una herramienta moderna que combina gestiÃ³n de entornos virtuales y resoluciÃ³n de dependencias de forma rÃ¡pida y eficiente.

Este enfoque reemplaza el uso tradicional de herramientas como `venv`, `pip` y `virtualenv`, ofreciendo una experiencia mÃ¡s simple y veloz.

â„¹ï¸ Para mÃ¡s detalles sobre cÃ³mo instalar y utilizar uv, consulta el archivo [`working-with-uv.md`](../docs/working-with-uv.md)

Una vez instalado `uv`, puedes crear el entorno virtual e instalar todas las dependencias necesarias con un solo comando:

```bash
uv venv && uv sync
```

Esto crearÃ¡ un entorno virtual en el directorio del proyecto y sincronizarÃ¡ las librerÃ­as especificadas en el archivo `pyproject.toml`.

### CÃ³mo crear una cuenta en OpenAI y configurar tu API Key para usarla con Python

#### 1. Crear una cuenta en OpenAI
- IngresÃ¡ a https://platform.openai.com/signup
- Registrate usando una cuenta de Google, Microsoft o un correo electrÃ³nico.
- VerificÃ¡ tu identidad mediante nÃºmero de telÃ©fono (es obligatorio).
- Una vez dentro, accedÃ© al Dashboard de OpenAI

#### 2. Obtener tu API Key
- En el dashboard, hacÃ© clic en "API keys" desde el menÃº izquierdo.
- PresionÃ¡ "Create new secret key".
- CopiÃ¡ la API Key generada (se muestra una sola vez). Guardala en un lugar seguro.

#### 3. Configurar la API Key como variable de entorno
En tu terminal (vÃ¡lido solo mientras la terminal estÃ© abierta)

```bash
export OPENAI_API_KEY=tu_api_key_aqui
```

âš ï¸ No incluyas tu API key directamente en el cÃ³digo si vas a subirlo a un repositorio o compartirlo.

#### 4. Instalar la libreria necesaria
InstalÃ¡ la librerÃ­a oficial de OpenAI:

```bash
uv add openai
# o
pip install openai
```

Con estos pasos, estÃ¡s listo para comenzar a interactuar con los modelos de OpenAI de manera segura y flexible desde Python.

### CÃ³digo Python que muestra cÃ³mo usar un LLM con una base de conocimiento externa (RAG)

Si querÃ©s ver un ejemplo prÃ¡ctico de cÃ³mo implementar RAG (Retrieval-Augmented Generation) utilizando OpenAI o Gemini y una base de conocimiento local, podÃ©s consultar el archivo [`llm_api_examples_gemini_openai`](./notebook/llm_api_examples_gemini_openai.ipynb)

Este notebook incluye ejemplos de cÃ³mo:
- Hacer una consulta simple a un modelo LLM (como GPT o Gemini).
- Conectar ese modelo a una fuente externa de datos (como un documento o colecciÃ³n de texto).
- Combinar recuperaciÃ³n de informaciÃ³n y generaciÃ³n de texto para responder preguntas con base en conocimiento personalizado.

## ðŸ”— Lectura recomendada
Recomendado para profundizar en los conceptos clave y ampliar tu comprensiÃ³n
* [Del prompt a la respuesta: el poder de los Large Language Models](https://medium.com/@j92riquelme/del-prompt-a-la-respuesta-el-poder-de-los-large-language-models-b4a28663fed9)
* [Â¿QuÃ© son los modelos de lenguaje de grandes (LLM)?](https://azure.microsoft.com/es-es/resources/cloud-computing-dictionary/what-are-large-language-models-llms)
* [Introduction to Large Language Models](https://developers.google.com/machine-learning/resources/intro-llms)
* [Â¿CÃ³mo lograr que una IA responda con precisiÃ³n sobre mis propios documentos? ConocÃ© RAG (Retrieval-Augmented Generation)](https://medium.com/@j92riquelme/cÃ³mo-lograr-que-una-ia-responda-con-precisiÃ³n-sobre-mis-propios-documentos-e7959a816cef)
* [Â¿QuÃ© es la generaciÃ³n aumentada por recuperaciÃ³n o RAG?](https://www.redhat.com/es/topics/ai/what-is-retrieval-augmented-generation?gad_source=1&gad_campaignid=22501758914&gbraid=0AAAAADsbVMTiqBq3YrdPWxHpN5RJa_5aL&gclid=CjwKCAjwl_XBBhAUEiwAWK2hzkNLesvpeuFH1oXLAuzgvPRbeY6Cf9pQ95r2zDw8ag-cCyKsUXdDghoCklcQAvD_BwE)
* [Â¿QuÃ© es la RAG (generaciÃ³n aumentada por recuperaciÃ³n)?](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/)
* [Â¿QuÃ© es la generaciÃ³n mejorada por recuperaciÃ³n (RAG)?](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=es_419)
* [Retrieval Augmented Generation (RAG) for LLMs](https://www.promptingguide.ai/research/rag)
* [Advanced RAG: Architecture, techniques, applications and use cases and development](https://www.leewayhertz.com/advanced-rag/)
* [Modelos de Lenguaje Extensos y Prompts](https://medium.com/@j92riquelme/modelos-de-lenguaje-extensos-y-prompts-339082864872)
* [OpenAI - Developer quickstart](https://platform.openai.com/docs/quickstart?api-mode=responses&lang=python)
* [API de Gemini Developer](https://ai.google.dev/gemini-api/docs?hl=es-419)
* [â€œAttention is All You Needâ€: La chispa que encendiÃ³ la revoluciÃ³n de la IA Generativa](https://medium.com/@j92riquelme/attention-is-all-you-need-la-chispa-que-encendi%C3%B3-la-revoluci%C3%B3n-de-la-ia-generativa-5c987353039b)
* [ReACT Agent Model](https://klu.ai/glossary/react-agent-model)
* [What is a ReAct agent?](https://www.ibm.com/think/topics/react-agent)
* [El ciclo de vida de un proyecto de IA: de la idea a la implementaciÃ³n](https://medium.com/@j92riquelme/el-ciclo-de-vida-de-un-proyecto-de-ia-de-la-idea-a-la-implementaciÃ³n-bae6794121f3)
* [Prompt engineering is the new feature engineering](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering)
* [Few-shot learning in practice: GPT-Neo and the Accelerated Inference API](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)
* [Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart)

## â–¶ï¸ Videos recomendados
SelecciÃ³n de videos para reforzar visualmente los temas abordados
* [IntroducciÃ³n a la IA generativa](https://www.youtube.com/watch?v=tNBvUvsScAA&t=1s)
* [IntroducciÃ³n a los modelos de lenguaje grandes](https://www.youtube.com/watch?v=Vi0ODh3ncxw&t=3s)
* [Introduction to Responsible AI](https://www.youtube.com/watch?v=JbluXe6QpxM&t=4s)
* [MIT 6.S191 (Google): Modelos de lenguaje grandes](https://www.youtube.com/watch?v=ZNodOsz94cc)


## ðŸ“š Cursos adicionales recomendados
Recursos complementarios para seguir aprendiendo y fortaleciendo tus habilidades.

* [Introduction to Generative AI Learning Path](https://www.cloudskillsboost.google/paths/118)
* [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)

---

> ðŸ“Œ **Nota:** este repositorio complementa el curso **LLM Zoomcamp** de [DataTalks.Club](https://datatalks.club/), y contiene notas, lecturas, videos, ejemplos y recursos adicionales.  
> Para acceder al contenido oficial del curso, visita el [**repositorio principal en GitHub**](https://github.com/DataTalksClub/llm-zoomcamp).
