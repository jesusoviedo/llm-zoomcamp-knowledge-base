# Semana 1 - LLM Zoomcamp

Este documento recopila mis apuntes y recursos para la **Semana 1** del curso LLM Zoomcamp.

## üìù Notas de la teor√≠a

### Large Language Model (LLM)

Un LLM (Large Language Model) es un modelo de lenguaje basado en redes neuronales profundas, entrenado para predecir el siguiente token (una palabra, parte de una palabra o s√≠mbolo) en una secuencia de texto. A partir de grandes vol√∫menes de datos textuales, aprende patrones sint√°cticos, sem√°nticos y contextuales, lo que le permite generar respuestas coherentes, relevantes y con apariencia humana.

Los LLM m√°s avanzados est√°n compuestos por miles de millones o incluso billones de par√°metros. Estos par√°metros ‚Äîlos pesos de la red neuronal‚Äî representan el conocimiento adquirido durante el entrenamiento.

El rendimiento de un LLM depende en gran medida de tres factores:
* La cantidad y diversidad del corpus de entrenamiento
* La escala del modelo (n√∫mero de par√°metros)
* Y su arquitectura, siendo el Transformer la base de los modelos modernos.

**¬øC√≥mo predice tokens un LLM?**
1. Se introduce un prompt o secuencia inicial de texto.
2. El modelo calcula la probabilidad de cada posible token que podr√≠a continuar la secuencia.
3. Se selecciona el token m√°s probable (o uno entre los m√°s probables, usando t√©cnicas como muestreo o top-k).
4. El token elegido se agrega al texto y el proceso se repite hasta completar la respuesta.

### Retrieval-Augmented Generation (RAG)

En el √°mbito de los Modelos de Lenguaje Grandes (LLMs), una de las limitaciones recurrentes ha sido su tendencia a "alucinar" o generar informaci√≥n incorrecta o no verificable. Aunque los LLMs son incre√≠blemente potentes para comprender y generar texto, su conocimiento se limita a los datos con los que fueron entrenados. Aqu√≠ es donde entra en juego **RAG (Retrieval-Augmented Generation)**, una t√©cnica que busca solucionar esta deficiencia, permitiendo a los LLMs acceder a fuentes de informaci√≥n externas y verificables en tiempo real, mejorando significativamente la precisi√≥n y relevancia de sus respuestas.

La idea central de RAG es combinar la capacidad generativa de un LLM con la capacidad de recuperaci√≥n de informaci√≥n de un sistema de b√∫squeda. En lugar de generar una respuesta bas√°ndose √∫nicamente en su entrenamiento interno, un modelo RAG primero busca y recupera documentos o pasajes relevantes de una base de datos externa, que puede ser tan vasta como se desee (documentaci√≥n interna de una empresa, una base de conocimientos, art√≠culos cient√≠ficos, etc.).

El proceso se desarrolla en dos fases principales. Primero, cuando un usuario plantea una pregunta o una consulta, esta se utiliza para buscar informaci√≥n en la base de datos externa. Esta b√∫squeda se realiza t√≠picamente utilizando t√©cnicas de incrustaci√≥n (embeddings) que transforman el texto de la consulta y los documentos en vectores num√©ricos, permitiendo medir la similitud sem√°ntica. Los documentos m√°s relevantes se recuperan para la siguiente etapa.

En la segunda fase, la informaci√≥n recuperada, junto con la consulta original del usuario, se alimenta al LLM. El LLM entonces utiliza este contexto adicional, que es fidedigno y espec√≠fico, para generar su respuesta. Este enfoque permite que el modelo genere respuestas m√°s precisas, fundamentadas en hechos y adaptadas a la informaci√≥n m√°s reciente o espec√≠fica que no estaba presente en sus datos de entrenamiento iniciales.

La principal ventaja de RAG es que reduce dr√°sticamente las alucinaciones y permite que los LLMs respondan preguntas sobre informaci√≥n muy espec√≠fica o datos que no exist√≠an en el momento de su entrenamiento. Es una soluci√≥n ideal para aplicaciones que requieren alta precisi√≥n y verificabilidad, como chatbots corporativos, asistentes de soporte t√©cnico o sistemas de preguntas y respuestas basados en bases de conocimientos. Adem√°s, al no requerir un reentrenamiento completo del LLM cada vez que se actualiza la base de conocimientos, RAG ofrece una soluci√≥n m√°s eficiente y escalable para mantener los LLMs actualizados y relevantes.

En resumen, RAG transforma los LLMs de ser meros generadores de texto basados en su memoria de entrenamiento a convertirse en potentes "investigadores" capaces de consultar y sintetizar informaci√≥n externa, brindando respuestas m√°s ricas, precisas y confiables. Es un paso crucial hacia LLMs verdaderamente informados y √∫tiles en una amplia variedad de aplicaciones.

![alt text](./img/image.png)

## üõ†Ô∏è Ejemplo pr√°ctico de RAG (Retrieval-Augmented Generation

### Creaci√≥n de un entorno de desarrollo

Para crear y gestionar el entorno de Python de este proyecto se utiliza `uv`, una herramienta moderna que combina gesti√≥n de entornos virtuales y resoluci√≥n de dependencias de forma r√°pida y eficiente.

Este enfoque reemplaza el uso tradicional de herramientas como `venv`, `pip` y `virtualenv`, ofreciendo una experiencia m√°s simple y veloz.

‚ÑπÔ∏è Para m√°s detalles sobre c√≥mo instalar y utilizar uv, consulta el archivo [`working-with-uv.md`](../docs/working-with-uv.md)

Una vez instalado `uv`, puedes crear el entorno virtual e instalar todas las dependencias necesarias con un solo comando:

```bash
uv venv && uv sync
```

Esto crear√° un entorno virtual en el directorio del proyecto y sincronizar√° las librer√≠as especificadas en el archivo `pyproject.toml`.

### C√≥mo crear una cuenta en OpenAI y configurar tu API Key para usarla con Python

#### 1. Crear una cuenta en OpenAI
- Ingres√° a https://platform.openai.com/signup
- Registrate usando una cuenta de Google, Microsoft o un correo electr√≥nico.
- Verific√° tu identidad mediante n√∫mero de tel√©fono (es obligatorio).
- Una vez dentro, acced√© al Dashboard de OpenAI

#### 2. Obtener tu API Key
- En el dashboard, hac√© clic en "API keys" desde el men√∫ izquierdo.
- Presion√° "Create new secret key".
- Copi√° la API Key generada (se muestra una sola vez). Guardala en un lugar seguro.

#### 3. Configurar la API Key como variable de entorno
En tu terminal (v√°lido solo mientras la terminal est√© abierta)

```bash
export OPENAI_API_KEY=tu_api_key_aqui
```

‚ö†Ô∏è No incluyas tu API key directamente en el c√≥digo si vas a subirlo a un repositorio o compartirlo.

#### 4. Instalar la libreria necesaria
Instal√° la librer√≠a oficial de OpenAI:

```bash
uv add openai
# o
pip install openai
```

Con estos pasos, est√°s listo para comenzar a interactuar con los modelos de OpenAI de manera segura y flexible desde Python.

### C√≥digo Python que muestra c√≥mo usar un LLM con una base de conocimiento externa (RAG)

Si quer√©s ver un ejemplo pr√°ctico de c√≥mo implementar RAG (Retrieval-Augmented Generation) utilizando OpenAI o Gemini y una base de conocimiento local, pod√©s consultar el archivo [`llm_api_examples_gemini_openai`](./notebook/llm_api_examples_gemini_openai.ipynb)

Este notebook incluye ejemplos de c√≥mo:
- Hacer una consulta simple a un modelo LLM (como GPT o Gemini).
- Conectar ese modelo a una fuente externa de datos (como un documento o colecci√≥n de texto).
- Combinar recuperaci√≥n de informaci√≥n y generaci√≥n de texto para responder preguntas con base en conocimiento personalizado.

## üîó Lectura recomendada
Recomendado para profundizar en los conceptos clave y ampliar tu comprensi√≥n
* [Del prompt a la respuesta: el poder de los Large Language Models](https://medium.com/@j92riquelme/del-prompt-a-la-respuesta-el-poder-de-los-large-language-models-b4a28663fed9)
* [¬øQu√© son los modelos de lenguaje de grandes (LLM)?](https://azure.microsoft.com/es-es/resources/cloud-computing-dictionary/what-are-large-language-models-llms)
* [Introduction to Large Language Models](https://developers.google.com/machine-learning/resources/intro-llms)
* [¬øC√≥mo lograr que una IA responda con precisi√≥n sobre mis propios documentos? Conoc√© RAG (Retrieval-Augmented Generation)](https://medium.com/@j92riquelme/c√≥mo-lograr-que-una-ia-responda-con-precisi√≥n-sobre-mis-propios-documentos-e7959a816cef)
* [¬øQu√© es la generaci√≥n aumentada por recuperaci√≥n o RAG?](https://www.redhat.com/es/topics/ai/what-is-retrieval-augmented-generation?gad_source=1&gad_campaignid=22501758914&gbraid=0AAAAADsbVMTiqBq3YrdPWxHpN5RJa_5aL&gclid=CjwKCAjwl_XBBhAUEiwAWK2hzkNLesvpeuFH1oXLAuzgvPRbeY6Cf9pQ95r2zDw8ag-cCyKsUXdDghoCklcQAvD_BwE)
* [¬øQu√© es la RAG (generaci√≥n aumentada por recuperaci√≥n)?](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/)
* [¬øQu√© es la generaci√≥n mejorada por recuperaci√≥n (RAG)?](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=es_419)
* [Retrieval Augmented Generation (RAG) for LLMs](https://www.promptingguide.ai/research/rag)
* [Advanced RAG: Architecture, techniques, applications and use cases and development](https://www.leewayhertz.com/advanced-rag/)
* [Modelos de Lenguaje Extensos y Prompts](https://medium.com/@j92riquelme/modelos-de-lenguaje-extensos-y-prompts-339082864872)
* [OpenAI - Developer quickstart](https://platform.openai.com/docs/quickstart?api-mode=responses&lang=python)
* [API de Gemini Developer](https://ai.google.dev/gemini-api/docs?hl=es-419)
* [‚ÄúAttention is All You Need‚Äù: La chispa que encendi√≥ la revoluci√≥n de la IA Generativa](https://medium.com/@j92riquelme/attention-is-all-you-need-la-chispa-que-encendi%C3%B3-la-revoluci%C3%B3n-de-la-ia-generativa-5c987353039b)
* [ReACT Agent Model](https://klu.ai/glossary/react-agent-model)
* [What is a ReAct agent?](https://www.ibm.com/think/topics/react-agent)
* [El ciclo de vida de un proyecto de IA: de la idea a la implementaci√≥n](https://medium.com/@j92riquelme/el-ciclo-de-vida-de-un-proyecto-de-ia-de-la-idea-a-la-implementaci√≥n-bae6794121f3)
* [Prompt engineering is the new feature engineering](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering)
* [Few-shot learning in practice: GPT-Neo and the Accelerated Inference API](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)
* [Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart)

## ‚ñ∂Ô∏è Videos recomendados
Selecci√≥n de videos para reforzar visualmente los temas abordados
* [Introducci√≥n a la IA generativa](https://www.youtube.com/watch?v=tNBvUvsScAA&t=1s)
* [Introducci√≥n a los modelos de lenguaje grandes](https://www.youtube.com/watch?v=Vi0ODh3ncxw&t=3s)
* [Introduction to Responsible AI](https://www.youtube.com/watch?v=JbluXe6QpxM&t=4s)
* [MIT 6.S191 (Google): Modelos de lenguaje grandes](https://www.youtube.com/watch?v=ZNodOsz94cc)


## üìö Cursos adicionales recomendados
Recursos complementarios para seguir aprendiendo y fortaleciendo tus habilidades.

* [Introduction to Generative AI Learning Path](https://www.cloudskillsboost.google/paths/118)
* [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)

---

> üìå **Nota:** este repositorio complementa el curso **LLM Zoomcamp** de [DataTalks.Club](https://datatalks.club/), y contiene notas, lecturas, videos, ejemplos y recursos adicionales.  
> Para acceder al contenido oficial del curso, visita el [**repositorio principal en GitHub**](https://github.com/DataTalksClub/llm-zoomcamp).
