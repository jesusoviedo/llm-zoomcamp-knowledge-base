{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7f03a2-6a40-4a00-a670-35ef86319b10",
   "metadata": {},
   "source": [
    "# RAG con Qdrant + OpenAI\n",
    "\n",
    "### Requisitos de instalación\n",
    "\n",
    "Antes de ejecutar el proyecto, asegurate de tener instaladas las siguientes librerías de Python:\n",
    "\n",
    "```bash\n",
    "pip install \\\n",
    "  fastembed>=0.7.1 \\\n",
    "  ipywidgets>=8.1.7 \\\n",
    "  notebook>=7.4.3 \\\n",
    "  openai>=1.93.0 \\\n",
    "  qdrant-client>=1.14.3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a0261-3e6f-4084-85f3-55a539fe63fe",
   "metadata": {},
   "source": [
    "###  Descarga y procesamiento de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc48a12-8e67-4f62-b396-767f1e2b289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "documents_raw = requests.get(docs_url).json()\n",
    "\n",
    "# Usamos una lista por comprensión para mayor velocidad y claridad\n",
    "documents = [\n",
    "    {**doc, 'course': course['course']}\n",
    "    for course in documents_raw\n",
    "    for doc in course['documents']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad64142-0316-4b3f-8013-e2ad64acb6f5",
   "metadata": {},
   "source": [
    "### Construcción de la colección en Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7749dc44-598f-4dfb-b91a-19b3423a6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "qd_client = QdrantClient(\"http://localhost\", port=6333)\n",
    "collection_name = \"zoomcamp-faq\"\n",
    "embedding_dim = 512\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "\n",
    "# Solo borra la colección si existe (evita error o espera innecesaria)\n",
    "if collection_name in [col.name for col in qd_client.get_collections().collections]:\n",
    "    qd_client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "# Usa create_collection para crear una collection\n",
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "# Usa create_payload_index solo si no existe\n",
    "existing_indexes = qd_client.get_collection(collection_name).payload_schema\n",
    "if \"course\" not in existing_indexes:\n",
    "    qd_client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"course\",\n",
    "        field_schema=\"keyword\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4c75b-eaa4-4680-9c01-aec084e88769",
   "metadata": {},
   "source": [
    "### Inserción de documentos vectorizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b78a57-7401-4582-b1ab-d3211518258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 948/948 [00:00<00:00, 393527.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import Document, PointStruct\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# Barra de progreso\n",
    "points = []\n",
    "\n",
    "def build_point(i, doc):\n",
    "    text = f\"{doc['question']} {doc['text']}\"\n",
    "    vector = Document(text=text, model=model_handle)\n",
    "    return PointStruct(id=i, vector=vector, payload=doc)\n",
    "\n",
    "# Generar puntos en paralelo (si hay muchos documentos)\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    points = list(tqdm(executor.map(lambda x: build_point(*x), enumerate(documents)), total=len(documents)))\n",
    "\n",
    "# Usar upsert en batches para reducir el uso de memoria y red\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "for i in range(0, len(points), BATCH_SIZE):\n",
    "    batch = points[i:i + BATCH_SIZE]\n",
    "    qd_client.upsert(collection_name=collection_name, points=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abefaa8-d60c-47ba-bccb-f69b4fb9655d",
   "metadata": {},
   "source": [
    "### Función de búsqueda vectorial con filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f7012-e1a7-4cca-a6c3-17fc296902ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(\n",
    "    question,\n",
    "    course,\n",
    "    limit,\n",
    "    model_handle,\n",
    "    collection_name\n",
    "):\n",
    "    print(f\"[vector_search] Searching for: '{question}' (course={course})\")\n",
    "\n",
    "    # Construcción explícita del filtro para claridad\n",
    "    search_filter = models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"course\",\n",
    "                match=models.MatchValue(value=course)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Generar vector para la pregunta (puede tardar)\n",
    "    query_vector = models.Document(text=question, model=model_handle)\n",
    "\n",
    "    # Consulta a Qdrant\n",
    "    query_result = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        query_filter=search_filter,\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    return [point.payload for point in query_result.points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64717733-af10-4ea9-9c69-9783c3663e69",
   "metadata": {},
   "source": [
    "### Construcción del prompt para el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd4f87-8c23-4bf7-9666-bd19b45d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = (\n",
    "        \"You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\\n\"\n",
    "        \"Use only the facts from the CONTEXT when answering the QUESTION.\\n\\n\"\n",
    "        \"QUESTION: {question}\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\"\n",
    "    )\n",
    "\n",
    "    # Usar join + list comprehension es más eficiente que concatenar strings\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\"\n",
    "        for doc in search_results\n",
    "    )\n",
    "\n",
    "    return prompt_template.format(question=query, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5097e-8c39-41b6-9e83-b06649aa0971",
   "metadata": {},
   "source": [
    "### Generación de respuestas con un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31009b11-c4ce-4852-a44a-ab033611aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAIError  # O el error correcto según tu cliente\n",
    "from openai import OpenAI\n",
    "\n",
    "def llm(prompt, model=\"gpt-4o-mini\", max_retries=3):\n",
    "\n",
    "    openai_client = OpenAI()\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except OpenAIError as e:\n",
    "            print(f\"[llm] Error (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            time.sleep(1.5 * (attempt + 1))\n",
    "\n",
    "    raise RuntimeError(\"LLM request failed after multiple attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e3355-5920-412c-8872-7ff05604af8a",
   "metadata": {},
   "source": [
    "### El pipeline completo: función rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de980b83-d5c9-4b2f-8d38-5216d147fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(\n",
    "    query,\n",
    "    course='data-engineering-zoomcamp',\n",
    "    limit=5,\n",
    "    model='gpt-4o-mini',\n",
    "    embedding_model='jinaai/jina-embeddings-v2-small-en',\n",
    "    collection_name='zoomcamp-faq'\n",
    "):\n",
    "    search_results = vector_search(\n",
    "        question=query,\n",
    "        course=course,\n",
    "        limit=limit,\n",
    "        model_handle=embedding_model,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"No relevant documents found to answer the question.\"\n",
    "\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "rag('how do I run kafka?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
