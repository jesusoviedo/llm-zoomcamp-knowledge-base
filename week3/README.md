# Semana 3 - LLM Zoomcamp

Este documento recopila mis apuntes y recursos para la **Semana 3** del curso LLM Zoomcamp.

## ðŸ“ Notas de la teorÃ­a

### Importancia de la EvaluaciÃ³n

Es fundamental evaluar los resultados de bÃºsqueda para optimizar cÃ³mo se almacenan y recuperan los datos en un sistema RAG (Retrieval-Augmented Generation). Sin una evaluaciÃ³n adecuada, es difÃ­cil saber si el sistema estÃ¡ devolviendo resultados Ãºtiles y relevantes. El rendimiento ideal siempre depende de los datos disponibles y los requisitos especÃ­ficos del caso de uso.

El objetivo es medir de forma objetiva quÃ© tan bueno es un sistema de bÃºsqueda. En lugar de confiar en la intuiciÃ³n, usamos mÃ©tricas para cuantificar el rendimiento. Esto nos permite comparar diferentes sistemas (como Elasticsearch o Qdrant vs. otro motor de bÃºsqueda) o diferentes configuraciones del mismo sistema.


### Datos de Referencia (Ground Truth)

Para evaluar un sistema de recuperaciÃ³n, se necesita un conjunto de datos de referencia (tambiÃ©n conocido como *ground truth* o *gold standard data*). Este conjunto contiene consultas junto con los documentos relevantes para cada una.

#### **Necesidad de un "Ground Truth"**
Es indispensable contar con este conjunto para evaluar de manera efectiva el rendimiento del sistema. Cada entrada deberÃ­a vincular una pregunta con el documento que contiene la respuesta correcta.

#### **Estructura del Conjunto de Datos**
Idealmente, el conjunto contiene miles de preguntas, y para cada una se identifican los documentos relevantes. En algunos casos, como simplificaciÃ³n, se asume que existe **un solo documento relevante por pregunta**.

#### **MÃ©todos para Crear Ground Truth**

- **AnotaciÃ³n Humana**: Especialistas etiquetan manualmente las preguntas con sus documentos relevantes. Es el mÃ©todo mÃ¡s preciso, aunque costoso y lento.
- **ObservaciÃ³n del Usuario**: Se analiza el comportamiento real de los usuarios. Puede ser evaluado por humanos o por LLMs.
- **GeneraciÃ³n AutomÃ¡tica con LLM**: Usar un modelo como GPT-4 para generar preguntas y asociarlas automÃ¡ticamente a documentos de referencia (por ejemplo, desde un conjunto de preguntas frecuentes).

#### **CreaciÃ³n de un ID de Documento Ãšnico**
Para asegurar trazabilidad, se genera un hash (MD5) con los campos del documento (curso, pregunta, texto), lo que garantiza que el ID cambie solo si el contenido cambia.

#### **Ejemplo de ImplementaciÃ³n**
- Se carga un conjunto de documentos.
- Se asigna un ID Ãºnico a cada uno.
- Se diseÃ±a un prompt detallado que instruye al LLM a actuar como un estudiante y generar cinco preguntas variadas por FAQ.
- El resultado es almacenado como un DataFrame y luego exportado a CSV.

#### **OptimizaciÃ³n de la BÃºsqueda**

Varios parÃ¡metros de bÃºsqueda pueden ajustarse para mejorar los resultados obtenidos. Algunos ejemplos incluyen:

- Tipo de bÃºsqueda: densa, hÃ­brida, BM25.
- Campos del documento que se consultan: tÃ­tulo, cuerpo, metadatos, etc.
- Uso de boosting para priorizar campos como `question`.
- Filtros por categorÃ­a, curso o tipo de contenido.

La evaluaciÃ³n es la herramienta clave para identificar quÃ© configuraciÃ³n produce el mejor rendimiento en funciÃ³n de las necesidades del usuario.

#### **MÃ©tricas de Ranking Comunes**

Para cuantificar la calidad de los resultados, se utilizan diversas mÃ©tricas. Algunas de las mÃ¡s comunes incluyen:

- PrecisiÃ³n en k (P@k)
- Recall
- Mean Average Precision (MAP)
- Normalized Discounted Cumulative Gain (NDCG)
- Mean Reciprocal Rank (MRR)
- F1 Score
- Area Under the ROC curve (AUC-ROC)
- Mean Rank (MR)
- Hit Rate (HR) o Recall at K
- Expected Reciprocal Rank (ERR)

Cada una de estas mÃ©tricas captura distintos aspectos del rendimiento del sistema, como la precisiÃ³n, el orden de los resultados o la cobertura.

### EvaluaciÃ³n y Monitoreo

Aunque los sistemas RAG utilizan modelos de lenguaje como parte del proceso, los LLMs tambiÃ©n pueden usarse **de manera independiente** en tareas como generaciÃ³n de texto, clasificaciÃ³n o razonamiento. Por eso, es importante entender cÃ³mo evaluarlos y monitorearlos en distintos contextos.

#### **EvaluaciÃ³n de LLMs: Â¿QuÃ© se mide y cÃ³mo?**

- **Calidad del contenido generado**  
  Se busca responder: Â¿es Ãºtil, coherente, relevante y veraz lo que produce el modelo? Algunas formas comunes de evaluar esto son:

  - **Human Evaluation**  
    Personas humanas califican las respuestas del modelo en dimensiones como claridad, utilidad, precisiÃ³n factual o tono.

  - **Automatic Metrics**  
    Herramientas que cuantifican la calidad de las respuestas:
    - `BLEU`, `ROUGE`, `METEOR`: comparan respuestas generadas con referencias.
    - `BERTScore`: mide similitud semÃ¡ntica entre textos.
    - **Similitud Embedding (coseno)**: se representa el texto en vectores y se mide su cercanÃ­a matemÃ¡tica.

  - **LLM-as-a-Judge**  
    Otro LLM actÃºa como evaluador. Se le da una instrucciÃ³n del tipo:
    "Dada esta pregunta y esta respuesta generada, Â¿quÃ© tan Ãºtil y correcta es la respuesta?"

  - **Pruebas A/B**  
    Se comparan dos versiones del modelo (por ejemplo, una con un nuevo prompt o ajuste de hiperparÃ¡metros) con usuarios reales. A cada grupo se le muestra una versiÃ³n diferente, y se recopilan mÃ©tricas como satisfacciÃ³n, tasa de clics o preferencia explÃ­cita entre respuestas.


- **EvaluaciÃ³n de comportamiento no deseado**  
  Los LLMs tambiÃ©n se evalÃºan para detectar errores como:
  - **Alucinaciones**: respuestas que suenan correctas pero son falsas.
  - **Bias o toxicidad**: lenguaje ofensivo o sesgado.
  - **Seguridad**: si el modelo responde a preguntas daÃ±inas o peligrosas.

#### **Monitoreo de LLMs en ProducciÃ³n**

Una vez que el LLM estÃ¡ en uso (en apps, asistentes, bots, etc.), el monitoreo continuo es clave para mantener su calidad y seguridad.

- **MÃ©tricas tÃ©cnicas**  
  - Latencia de respuesta  
  - Tasa de error o caÃ­das del sistema  
  - Costo por consulta (tokens, tiempo de cÃ³mputo)

- **MÃ©tricas de uso y comportamiento**  
  - NÃºmero de usuarios activos  
  - Tasa de Ã©xito en tareas (ejemplo: generaciÃ³n correcta de reportes, respuestas Ãºtiles)  
  - DetecciÃ³n de respuestas problemÃ¡ticas o alucinaciones en tiempo real

- **RetroalimentaciÃ³n del usuario**  
  - Pulgar arriba/abajo  
  - Comentarios abiertos  
  - NPS (Net Promoter Score) sobre la experiencia con el modelo

#### **Importancia de la EvaluaciÃ³n y el Monitoreo Combinados**

Un buen LLM no solo debe pasar pruebas antes de salir a producciÃ³n, sino tambiÃ©n demostrar que **mantiene su rendimiento bajo condiciones reales** y con usuarios diversos. La combinaciÃ³n de evaluaciÃ³n sistemÃ¡tica y monitoreo activo permite:

- Detectar degradaciones en la calidad.
- Responder rÃ¡pidamente a incidentes de seguridad.
- Identificar oportunidades para mejorar el modelo o ajustar prompts.

Evaluar y monitorear LLMs es un proceso continuo, que combina **pruebas offline** (en entornos controlados antes del despliegue, como benchmarks o datasets con ground truth), **pruebas online** (con usuarios reales y mÃ©tricas en tiempo real), **recolecciÃ³n de datos de uso** y **herramientas automÃ¡ticas** (como evaluadores automÃ¡ticos o LLM-as-a-Judge), para garantizar calidad, seguridad y valor para el usuario final.

#### **EvaluaciÃ³n Offline de Sistemas RAG**

La **evaluaciÃ³n offline** en sistemas de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) es esencial para validar la **calidad** de las respuestas **antes del despliegue en producciÃ³n**. Este proceso permite comparar modelos, ajustar configuraciones y optimizar el balance entre **precisiÃ³n, velocidad y costo**.

**Componentes de un sistema RAG**

Un sistema RAG consta de tres componentes clave:

1. **BÃºsqueda (Retrieval):**  
   Recupera documentos relevantes utilizando bÃºsquedas semÃ¡nticas en bases vectoriales como **Qdrant**, **Elasticsearch** o **FAISS**.

2. **ConstrucciÃ³n del prompt:**  
   Ensambla la consulta del usuario con los documentos recuperados, generando una entrada coherente para el modelo.

3. **GeneraciÃ³n con LLM:**  
   El modelo de lenguaje procesa el prompt y genera una respuesta final basada en el contexto aportado.

**Proceso de evaluaciÃ³n**

La evaluaciÃ³n offline simula interacciones antes de poner el sistema frente a usuarios reales. Utiliza **datasets de referencia** para medir el desempeÃ±o con mÃ©tricas cuantitativas. A continuaciÃ³n se explica un proceso paso a paso:

1. **IndexaciÃ³n de datos:**  
   Los documentos son convertidos en vectores con modelos como `multi-qa-MiniLM-L6-cos-v1`.

2. **GeneraciÃ³n de preguntas sintÃ©ticas (opcional):**  
   A partir de las respuestas ideales (ground truth), un LLM genera una pregunta coherente.

3. **ObtenciÃ³n de respuesta generada:**  
   Se evalÃºa cÃ³mo responde el modelo cuando recibe esa pregunta.

4. **ComparaciÃ³n entre respuestas (A vs Aâ€™):**  
   Se mide cuÃ¡n parecida es la respuesta generada (A') a la ideal (A), usando mÃ©tricas como:

   - **Similitud del coseno:**  
     EvalÃºa la cercanÃ­a semÃ¡ntica entre vectores de texto.  
     - `1.0`: muy similares  
     - `0.0`: completamente diferentes

   - **Otras mÃ©tricas complementarias:**  
     - `Exact Match`  
     - `F1 Score`  
     - `BLEU`, `ROUGE` (Ãºtiles cuando no se trabaja con embeddings)

**Comparativa de modelos**

La elecciÃ³n del modelo LLM no debe hacerse de forma arbitraria. Existen mÃºltiples opciones disponibles, y cada una presenta ventajas y desventajas en tÃ©rminos de **costo**, **tiempo de procesamiento** y **calidad de resultados**. Evaluar estos factores permite seleccionar el modelo mÃ¡s adecuado segÃºn el contexto y los objetivos del sistema RAG.

**Caso PrÃ¡ctico: ComparaciÃ³n de Modelos LLM de OpenAI**

Se compararon tres modelos populares para ver cuÃ¡l ofrece mejor relaciÃ³n **calidad / costo / tiempo**:

| Modelo         | Similitud promedio | Costo aprox. por lote | Tiempo de procesamiento |
|----------------|--------------------|------------------------|--------------------------|
| **GPT-4o Mini** | 0.683              | $0.10                  | 6.5 minutos (con multithreading) |
| **GPT-4**       | 0.679              | $10.00                 | 3 horas                  |
| **GPT-3.5 Turbo**| 0.657              | $0.79                  | 6.5 minutos              |

**Observaciones claves:**

- Las puntuaciones de calidad son **muy similares**.
- **GPT-4o Mini** destaca por ser **mÃ¡s rÃ¡pido y econÃ³mico**, aunque con restricciones de **rate limit**.
- Con tÃ©cnicas como `ThreadPoolExecutor` se puede acelerar el proceso sin comprometer calidad.

**VisualizaciÃ³n y MÃ©tricas**

Se recomienda visualizar los resultados con herramientas como:

- `sns.histplot()` para histogramas de similitud
- `sns.kdeplot()` para estimaciones de densidad
- `sns.ecdfplot()` para distribuciÃ³n acumulada

**Herramientas Ãºtiles**

- **RAGAS**: EvalÃºa mÃºltiples dimensiones (fidelidad, relevancia, recuerdo)
- **LangChain Evaluation**: Framework flexible para comparar cadenas y prompts
- **Pandas**: GestiÃ³n estructurada de resultados
- **tqdm**: VisualizaciÃ³n del progreso
- **PromptLayer** y **Evidently**: Para trazabilidad y anÃ¡lisis en producciÃ³n

La evaluaciÃ³n offline no solo permite elegir el mejor modelo, sino tambiÃ©n tomar decisiones informadas que optimicen el rendimiento general del sistema RAG.

> **Evaluar no es un paso Ãºnico, es un proceso continuo.**

#### **LLM como Juez en la EvaluaciÃ³n Offline y Online**

Cuando no se dispone de una respuesta ideal (ground truth), o se busca un enfoque mÃ¡s cualitativo, puede emplearse la tÃ©cnica **LLM-as-a-Judge**, donde otro modelo LLM evalÃºa las respuestas generadas.

**Escenario 1: EvaluaciÃ³n Offline (con ground truth)**

- **Contexto:** EvaluaciÃ³n previa al despliegue, en entorno controlado.
- **Datos disponibles para el LLM juez:**
  - Pregunta del usuario.
  - Respuesta generada por el sistema.
  - Respuesta ideal (ground truth).
- **Tarea del juez:** Clasificar la respuesta generada como `RELEVANTE`, `PARCIALMENTE RELEVANTE` o `NO RELEVANTE`, explicando su razonamiento.

**Escenario 2: EvaluaciÃ³n Online (sin ground truth)**

- **Contexto:** EvaluaciÃ³n en producciÃ³n, con datos en tiempo real.
- **Datos disponibles para el LLM juez:**
  - Pregunta del usuario.
  - Respuesta generada por el sistema.
- **Tarea del juez:** Evaluar si la respuesta es coherente y Ãºtil, sin referencia directa. Ideal para monitoreo continuo o testing en vivo.

**Ventaja:** Esta tÃ©cnica permite realizar evaluaciones mÃ¡s subjetivas o de criterio experto, incluso cuando no se dispone de respuestas ideales.

**DiseÃ±o de Prompts para el Juez LLM**

Para asegurar una evaluaciÃ³n consistente y Ãºtil, el LLM juez necesita instrucciones claras. Se recomienda usar prompts como:

- **Prompt para EvaluaciÃ³n Offline:**  
  `"Analiza la respuesta generada en relaciÃ³n con la respuesta original. ClasifÃ­cala como RELEVANTE, PARCIALMENTE RELEVANTE o NO RELEVANTE. Justifica tu elecciÃ³n."`

- **Prompt para EvaluaciÃ³n Online:**  
  `"EvalÃºa si la respuesta proporcionada es relevante para la pregunta. ClasifÃ­cala como RELEVANTE, PARCIALMENTE RELEVANTE o NO RELEVANTE. Justifica tu decisiÃ³n."`

**Consejo tÃ©cnico:** Estructura las respuestas del juez en formato **JSON** para facilitar su procesamiento automÃ¡tico.

**Lecciones Clave y Consideraciones TÃ©cnicas**

- **Las respuestas incorrectas son oro:** Las clasificaciones como "NO RELEVANTE" ayudan a detectar fallos crÃ­ticos en el sistema RAG (por ejemplo, recuperaciÃ³n errÃ³nea o prompts mal construidos).
- **IdentificaciÃ³n de cuellos de botella:** El anÃ¡lisis cualitativo permite detectar si el problema radica en la bÃºsqueda, construcciÃ³n del prompt o generaciÃ³n del LLM.
- **Monitoreo continuo:** En entornos productivos, el LLM juez permite mantener control de calidad sin intervenciÃ³n humana directa.

### Entendiendo las MÃ©tricas

Evaluar un sistema de recuperaciÃ³n o recomendaciÃ³n no solo implica observar si los resultados son buenos, sino tambiÃ©n **cuÃ¡n buenos**, **cuÃ¡n rÃ¡pidos** y **cuÃ¡n completos** son. Para eso, se utilizan diversas mÃ©tricas, cada una con su propia lÃ³gica y aplicaciones. A continuaciÃ³n, exploramos las mÃ¡s relevantes.

#### PrecisiÃ³n en k (P@k)

Mide la proporciÃ³n de documentos relevantes dentro de los primeros **k** resultados.

* **FÃ³rmula:**  
  $P@k = \frac{\text{NÃºmero de documentos relevantes en los primeros k resultados}}{k}$

**ExplicaciÃ³n Detallada**

Esta es la mÃ©trica mÃ¡s intuitiva. Imagina que buscas algo en Google y solo miras los primeros 10 resultados. La **PrecisiÃ³n en 10 (P@10)** te dice quÃ© porcentaje de esos 10 enlaces fue realmente Ãºtil. No le importa si habÃ­a mÃ¡s resultados buenos en la pÃ¡gina 2; solo se enfoca en la calidad de la primera "vitrina" de resultados que ve el usuario.

**Casos de Uso Comunes**

- **Motores de bÃºsqueda web (Google, Bing):** La mayorÃ­a de los usuarios no pasa de la primera pÃ¡gina.
- **BÃºsqueda en e-commerce (Amazon):** Si buscas "zapatillas rojas", quieres ver zapatillas rojas inmediatamente.
- **Sistemas de recomendaciÃ³n (Netflix, Spotify):** Es crucial que las primeras recomendaciones sean atractivas.

#### Recall (Exhaustividad o Sensibilidad)

Mide la proporciÃ³n de documentos relevantes que el sistema logrÃ³ encontrar de entre el **total** de documentos relevantes que existen.

* **FÃ³rmula:**  
  $Recall = \frac{\text{NÃºmero de documentos relevantes recuperados}}{\text{NÃºmero total de documentos relevantes}}$

**ExplicaciÃ³n Detallada**

El Recall se enfoca en la **cobertura**. No le importa si los resultados buenos estÃ¡n al principio o al final, solo quiere saber si el sistema fue capaz de encontrarlos todos. El objetivo es minimizar los "falsos negativos", es decir, no dejar fuera nada importante.

**Casos de Uso Comunes**

- **BÃºsquedas legales o de patentes:** Es crÃ­tico encontrar todos los documentos pertinentes.
- **DiagnÃ³stico mÃ©dico:** Se requiere considerar todas las posibilidades.
- **DetecciÃ³n de fraude o amenazas:** Es preferible detectar de mÃ¡s (falsos positivos) que pasar por alto algo grave.

#### PuntuaciÃ³n F1 (F1 Score)

Es la media armÃ³nica de la PrecisiÃ³n y el Recall. Busca un equilibrio entre ambos.

* **FÃ³rmula:**  
  $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$

**ExplicaciÃ³n Detallada**

La puntuaciÃ³n F1 es el gran pacificador entre PrecisiÃ³n y Recall. Es imposible obtener un F1 alto si una de las dos mÃ©tricas es muy baja. La media armÃ³nica penaliza los extremos, obligando al sistema a ser balanceado.

**Casos de Uso Comunes**

- **ClasificaciÃ³n de texto y anÃ¡lisis de sentimiento.**
- **EvaluaciÃ³n general de un sistema:** Proporciona una Ãºnica cifra para resumir rendimiento.


#### Rango RecÃ­proco Medio (Mean Reciprocal Rank - MRR)

EvalÃºa la posiciÃ³n en el ranking del **primer** documento relevante encontrado.

* **FÃ³rmula:**  
  $MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$  
  - $|Q|$ es el nÃºmero total de bÃºsquedas.  
  - $rank_i$ es la posiciÃ³n del primer resultado correcto para la bÃºsqueda $i$.

**ExplicaciÃ³n Detallada**

Â¿El sistema encontrÃ³ una buena respuesta rÃ¡pido? Si la respuesta correcta aparece primero, el puntaje es 1. En la segunda posiciÃ³n, 1/2. En la tercera, 1/3, y asÃ­ sucesivamente. El MRR promedia esta eficiencia a lo largo de muchas bÃºsquedas.

**Casos de Uso Comunes**

- **Sistemas de preguntas y respuestas (FAQs).**
- **Chatbots de soporte.**
- **Autocompletado o resultados tipo "Voy a tener suerte".**

#### Ganancia Cumulativa Descontada Normalizada (NDCG)

Mide la utilidad (o ganancia) de un documento segÃºn su posiciÃ³n en la lista de resultados.

* **FÃ³rmula:**  
  $NDCG = \frac{DCG}{IDCG}$  
  - **DCG:** suma la relevancia de cada documento, pero â€œdescuentaâ€ su valor cuanto mÃ¡s abajo estÃ©.  
  - **IDCG:** es el DCG ideal (orden perfecto).

**ExplicaciÃ³n Detallada**

NDCG es una de las mÃ©tricas mÃ¡s completas:
1. **Reconoce niveles de relevancia.**
2. **Penaliza posiciones bajas.**

El resultado normalizado varÃ­a entre 0 y 1, donde 1 representa un ranking perfecto.

**Casos de Uso Comunes**

- **Motores de bÃºsqueda y sistemas de recomendaciÃ³n.**
- **ComparaciÃ³n de modelos en competiciones y benchmarks.**

#### PrecisiÃ³n Media Promedio (Mean Average Precision - MAP)

Calcula la precisiÃ³n promedio para cada bÃºsqueda y luego promedia estos valores entre todas.

* **FÃ³rmula:**  
  $MAP = \frac{1}{|Q|} \sum_{q \in Q} \text{Average Precision(q)}$

**ExplicaciÃ³n Detallada**

El MAP recompensa que los documentos relevantes estÃ©n lo mÃ¡s arriba posible en los resultados. Aunque no diferencia grados de relevancia, mide bien el orden general de los aciertos.

**Casos de Uso Comunes**

- **BÃºsqueda de imÃ¡genes o documentos.**
- **Benchmarks acadÃ©micos en recuperaciÃ³n de informaciÃ³n.**

#### Tasa de Aciertos (Hit Rate - HR)

Mide la proporciÃ³n de bÃºsquedas donde se recuperÃ³ **al menos un** documento relevante en el top **k**.

* **FÃ³rmula:**  
  $HR@k = \frac{\text{NÃºmero de bÃºsquedas con al menos un acierto en el top k}}{|Q|}$

**ExplicaciÃ³n Detallada**

Es una mÃ©trica de â€œtodo o nadaâ€: Â¿hubo al menos un resultado Ãºtil en el top **k**? No se preocupa por cuÃ¡ntos, solo por si hubo al menos uno.

**Casos de Uso Comunes**

- **Sistemas de recomendaciÃ³n.**
- **ValidaciÃ³n de sistemas simples de recuperaciÃ³n.**


#### Rango RecÃ­proco Esperado (Expected Reciprocal Rank - ERR)

EvalÃºa la calidad del ranking considerando **la probabilidad de que un usuario encuentre una respuesta satisfactoria** en una posiciÃ³n determinada. A diferencia de MRR, tiene en cuenta mÃºltiples niveles de relevancia y la probabilidad de que un usuario deje de buscar despuÃ©s de ver un resultado Ãºtil.

* **FÃ³rmula (versiÃ³n simplificada):**  
  $ERR = \sum_{r=1}^{n} \frac{1}{r} \cdot P(r)$  
  - $P(r)$ es la probabilidad de que el usuario se detenga en la posiciÃ³n $r$ (basada en la relevancia del documento en esa posiciÃ³n).

**ExplicaciÃ³n Detallada**

ERR modela un comportamiento **mÃ¡s realista del usuario**: si encuentra un documento muy relevante en la primera posiciÃ³n, probablemente no siga mirando el resto. Pero si el documento es poco Ãºtil, seguirÃ¡ bajando.

Esta mÃ©trica se basa en la idea de que la relevancia no es binaria (sÃ­/no), sino que puede tener diferentes niveles (por ejemplo: irrelevante, relevante, muy relevante). ERR combina esto con una penalizaciÃ³n por posiciÃ³n, similar a NDCG, pero con una interpretaciÃ³n probabilÃ­stica.

**Casos de Uso Comunes**

- **Sistemas con mÃºltiples grados de relevancia** como motores de bÃºsqueda que clasifican resultados por utilidad.
- **Evaluaciones centradas en el usuario**, donde importa no solo quÃ© tan relevante es un documento, sino tambiÃ©n cuÃ¡ndo aparece en la lista.
- **Benchmarks avanzados**, como en competiciones de ranking (por ejemplo, TREC o LETOR).

---

Cuando se quiere evaluar automÃ¡ticamente la calidad de textos generados por un modelo, es comÃºn comparar esos textos con respuestas de referencia utilizando mÃ©tricas especÃ­ficas. A continuaciÃ³n, se explican algunas de las mÃ¡s utilizadas.

#### BLEU (Bilingual Evaluation Understudy)

EvalÃºa la calidad de un texto generado comparando **n-gramas coincidentes** (secuencias de palabras) con una o mÃ¡s referencias.

* **Idea clave:** Si el modelo usa muchas frases similares a las de la referencia, la calidad es alta.
* **PenalizaciÃ³n por longitud:** BLEU incluye una penalizaciÃ³n si el texto generado es mucho mÃ¡s corto que la referencia.

**FÃ³rmula (simplificada):**  
`BLEU = BP Ã— exp(âˆ‘ wâ‚™ Ã— log(pâ‚™))`  
- `BP`: *brevity penalty*  
- `pâ‚™`: precisiÃ³n de n-gramas de orden `n`  
- `wâ‚™`: peso asignado a cada orden (por ejemplo, 0.25 si se usan 4-gramas)

**Casos de Uso Comunes**  
- TraducciÃ³n automÃ¡tica  
- GeneraciÃ³n de texto donde se espera que el output se acerque mucho a una referencia  

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

Mide la **superposiciÃ³n de n-gramas o frases** entre la salida del modelo y una o varias referencias.

* **Variantes comunes:**
  - `ROUGE-1`: superposiciÃ³n de palabras individuales
  - `ROUGE-2`: superposiciÃ³n de bigramas (pares de palabras)
  - `ROUGE-L`: longitud de la subsecuencia comÃºn mÃ¡s larga (*Longest Common Subsequence*)

**Enfoque en recall:** A diferencia de BLEU (que prioriza precisiÃ³n), ROUGE estÃ¡ orientado a capturar cuÃ¡nta informaciÃ³n relevante se ha recuperado.

**Casos de Uso Comunes**  
- EvaluaciÃ³n de resÃºmenes automÃ¡ticos  
- Tareas donde es mÃ¡s importante **capturar lo esencial** del contenido  

#### METEOR (Metric for Evaluation of Translation with Explicit ORdering)

Mejora algunas limitaciones de BLEU y ROUGE utilizando:
- Coincidencias por sinÃ³nimos y variantes morfolÃ³gicas
- Penalizaciones por orden incorrecto
- EvaluaciÃ³n a nivel de frase

**Ventajas:**  
- Mayor correlaciÃ³n con evaluaciones humanas  
- Soporta flexibilidad lingÃ¼Ã­stica  

**Casos de Uso Comunes**  
- TraducciÃ³n automÃ¡tica  
- GeneraciÃ³n de lenguaje natural en contextos sensibles a la semÃ¡ntica  

#### BERTScore

EvalÃºa la **similitud semÃ¡ntica** entre el texto generado y la referencia utilizando **representaciones vectoriales obtenidas con BERT** (o modelos similares).

* **CÃ³mo funciona:**
  - Convierte cada palabra en un vector (*embedding*)
  - Compara los vectores del texto generado con los de la referencia
  - Usa mÃ©tricas de similitud como el coseno

**Ventajas:**  
- Capta mejor el *significado*, no solo la coincidencia superficial de palabras  
- Funciona bien en tareas donde el lenguaje puede ser diverso pero el mensaje debe mantenerse  

**Casos de Uso Comunes**  
- Resumen automÃ¡tico  
- Parafraseo  
- GeneraciÃ³n abierta de texto donde puede haber muchas respuestas correctas  

Estas mÃ©tricas ofrecen una primera aproximaciÃ³n rÃ¡pida y cuantitativa a la calidad del texto generado. Sin embargo, en muchos casos **es necesario complementarlas con evaluaciones humanas o LLM-as-a-Judge** para obtener una visiÃ³n completa.



## ðŸ› ï¸ Ejemplo prÃ¡ctico: cÃ³mo medir el rendimiento de tu sistema RAG

### CreaciÃ³n de un entorno de desarrollo

Para crear y gestionar el entorno de Python de este proyecto se utiliza `uv`, una herramienta moderna que combina la gestiÃ³n de entornos virtuales y la resoluciÃ³n de dependencias de forma rÃ¡pida y eficiente.

Este enfoque reemplaza el uso tradicional de herramientas como `venv`, `pip` y `virtualenv`, ofreciendo una experiencia mÃ¡s simple y Ã¡gil.

â„¹ï¸ Para mÃ¡s detalles sobre cÃ³mo instalar y utilizar uv, consulta el archivo [`working-with-uv.md`](../docs/working-with-uv.md)

Una vez instalado `uv`, puedes crear el entorno virtual e instalar todas las dependencias necesarias con un solo comando:

```bash
uv venv && uv sync
```

Este comando crearÃ¡ un entorno virtual en el directorio del proyecto y sincronizarÃ¡ las librerÃ­as especificadas en el archivo `pyproject.toml`.

### Evaluando la RecuperaciÃ³n de InformaciÃ³n en Sistemas RAG

#### **1. GeneraciÃ³n de datos de referencia (Ground Truth)**

Si deseas ver un ejemplo prÃ¡ctico de cÃ³mo generar datos de referencia para evaluar un sistema RAG, puedes consultar el archivo [`ground-truth-data.ipynb`](./notebook/ground-truth-data.ipynb).

Este notebook incluye ejemplos de:
- GeneraciÃ³n de datos de referencia (ground truth) con ayuda de un LLM.
- OrganizaciÃ³n de las consultas y documentos relevantes.
- PreparaciÃ³n de los datos para su posterior evaluaciÃ³n.
- Opcionalmente, exportaciÃ³n a un formato binario para su uso en experimentos posteriores.

Este paso es fundamental para contar con un conjunto sÃ³lido de datos antes de realizar la indexaciÃ³n.

#### **2. EvaluaciÃ³n de respuestas generadas (Text Evaluation)**

Si deseas ver un ejemplo prÃ¡ctico de cÃ³mo evaluar respuestas generadas por un sistema RAG, puedes consultar el archivo [`evaluate-text.ipynb`](./notebook/evaluate-text.ipynb).

Este notebook incluye ejemplos de:
- EvaluaciÃ³n automÃ¡tica de respuestas usando mÃ©tricas como MRR (Mean Reciprocal Rank).
- ComparaciÃ³n entre diferentes mÃ©todos para calcular la mÃ©trica.
- RevisiÃ³n de casos con fallos y aciertos para entender el rendimiento del sistema.
- PreparaciÃ³n de los resultados para visualizaciÃ³n o anÃ¡lisis posteriores.

Este paso es clave para medir la efectividad del sistema RAG a partir de los datos de referencia generados previamente.


## ðŸ”— Lectura recomendada
Recomendado para profundizar en los conceptos clave y ampliar tu comprensiÃ³n

* [Evaluating RAG Architectures on Benchmark Tasks](https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/comparing_techniques.html)
* [Best Practices for LLM Evaluation of RAG Applications](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)
* [Awesome RAG Evaluation](https://github.com/YHPeter/Awesome-RAG-Evaluation?tab=readme-ov-file)
* [An Overview on RAG Evaluation](https://weaviate.io/blog/rag-evaluation)
* [La guÃ­a definitiva para evaluar los componentes del sistema RAG: lo que necesitas saber](https://myscale.com/blog/es/ultimate-guide-to-evaluate-rag-system/)
* [Evaluadores de generaciÃ³n aumentada por recuperaciÃ³n (RAG)](https://learn.microsoft.com/es-es/azure/ai-foundry/concepts/evaluation-evaluators/rag-evaluators)
* [Let's talk about LLM evaluation](https://huggingface.co/blog/clefourrier/llm-evaluation)
* [RAG Evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation)
* [RAG Evaluation: Donâ€™t let customers tell you first](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/)
* [LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#:~:text=1,is%20able%20to%20call%20the)
* [Evaluating model performance](https://platform.openai.com/docs/guides/evals)
* [EvaluaciÃ³n de LLMs: Principales benchmarks y cÃ³mo entenderlos](https://www.nerds.ai/blog/evaluacion-de-llms-principales-benchmarks-y-como-entenderlos)
* [Ragas core Concepts](https://docs.ragas.io/en/stable/concepts/)
* [SentenceTransformers Documentation](https://sbert.net/)
* [SentenceTransformers Quickstart](https://sbert.net/docs/quickstart.html)
* [SentenceTransformers Pretrained Modelsïƒ](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)
* [LLM evaluation: a beginner's guide](https://www.evidentlyai.com/llm-guide/llm-evaluation)
* [La Arquitectura de la Confianza: Una GuÃ­a para la EvaluaciÃ³n de Sistemas de IA, LLM y RAG](https://medium.com/@j92riquelme/la-arquitectura-de-la-confianza-una-guia-para-la-evaluacion-de-sistemas-de-ia-be5be4aed808)


## â–¶ï¸ Videos recomendados
SelecciÃ³n de videos para reforzar visualmente los temas abordados

* [LLM evaluation for builders - Course announcement](https://www.youtube.com/watch?v=jQgI8tTkWQU&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d)
* [Welcome to the LLM evaluation course](https://www.youtube.com/watch?v=rHs0sP7b5fM&list=PL9omX6impEuMgDFCK_NleIB0sMzKs2boI)


## ðŸ“š Cursos adicionales recomendados
Recursos complementarios para seguir aprendiendo y fortaleciendo tus habilidades.

* [Ragas tutorials](https://docs.ragas.io/en/stable/getstarted/)
* [LLM apps: Evaluation](https://wandb.ai/site/courses/evals/?utm_source=chatgpt.com)
* [Building and Evaluating Advanced RAG Applications](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?utm_source=chatgpt.com)
* [EvaluaciÃ³n de modelos de lenguaje con Azure Databricks](https://learn.microsoft.com/es-es/training/modules/evaluate-language-models-azure-databricks/?utm_source=chatgpt.com)

---

> ðŸ“Œ **Nota:** este repositorio complementa el curso **LLM Zoomcamp** de [DataTalks.Club](https://datatalks.club/), y contiene notas, lecturas, videos, ejemplos y recursos adicionales.  
> Para acceder al contenido oficial del curso, visita el [**repositorio principal en GitHub**](https://github.com/DataTalksClub/llm-zoomcamp).
