# Semana 3 - LLM Zoomcamp

Este documento recopila mis apuntes y recursos para la **Semana 3** del curso LLM Zoomcamp.

## üìù Notas de la teor√≠a

### Importancia de la Evaluaci√≥n

Es fundamental evaluar los resultados de b√∫squeda para optimizar c√≥mo se almacenan y recuperan los datos en un sistema RAG (Retrieval-Augmented Generation). Sin una evaluaci√≥n adecuada, es dif√≠cil saber si el sistema est√° devolviendo resultados √∫tiles y relevantes. El rendimiento ideal siempre depende de los datos disponibles y los requisitos espec√≠ficos del caso de uso.

El objetivo es medir de forma objetiva qu√© tan bueno es un sistema de b√∫squeda. En lugar de confiar en la intuici√≥n, usamos m√©tricas para cuantificar el rendimiento. Esto nos permite comparar diferentes sistemas (como Elasticsearch o Qdrant vs. otro motor de b√∫squeda) o diferentes configuraciones del mismo sistema.


### Datos de Referencia (Ground Truth)

Para evaluar un sistema de recuperaci√≥n, se necesita un conjunto de datos de referencia (tambi√©n conocido como *ground truth* o *gold standard data*). Este conjunto contiene consultas junto con los documentos relevantes para cada una.

#### Necesidad de un "Ground Truth"
Es indispensable contar con este conjunto para evaluar de manera efectiva el rendimiento del sistema. Cada entrada deber√≠a vincular una pregunta con el documento que contiene la respuesta correcta.

#### Estructura del Conjunto de Datos
Idealmente, el conjunto contiene miles de preguntas, y para cada una se identifican los documentos relevantes. En algunos casos, como simplificaci√≥n, se asume que existe **un solo documento relevante por pregunta**.

#### M√©todos para Crear Ground Truth

- **Anotaci√≥n Humana**: Especialistas etiquetan manualmente las preguntas con sus documentos relevantes. Es el m√©todo m√°s preciso, aunque costoso y lento.
- **Observaci√≥n del Usuario**: Se analiza el comportamiento real de los usuarios. Puede ser evaluado por humanos o por LLMs.
- **Generaci√≥n Autom√°tica con LLM**: Usar un modelo como GPT-4 para generar preguntas y asociarlas autom√°ticamente a documentos de referencia (por ejemplo, desde un conjunto de preguntas frecuentes).

#### Creaci√≥n de un ID de Documento √önico
Para asegurar trazabilidad, se genera un hash (MD5) con los campos del documento (curso, pregunta, texto), lo que garantiza que el ID cambie solo si el contenido cambia.

#### Ejemplo de Implementaci√≥n
- Se carga un conjunto de documentos.
- Se asigna un ID √∫nico a cada uno.
- Se dise√±a un prompt detallado que instruye al LLM a actuar como un estudiante y generar cinco preguntas variadas por FAQ.
- El resultado es almacenado como un DataFrame y luego exportado a CSV.

#### Optimizaci√≥n de la B√∫squeda

Varios par√°metros de b√∫squeda pueden ajustarse para mejorar los resultados obtenidos. Algunos ejemplos incluyen:

- Tipo de b√∫squeda: densa, h√≠brida, BM25.
- Campos del documento que se consultan: t√≠tulo, cuerpo, metadatos, etc.
- Uso de boosting para priorizar campos como `question`.
- Filtros por categor√≠a, curso o tipo de contenido.

La evaluaci√≥n es la herramienta clave para identificar qu√© configuraci√≥n produce el mejor rendimiento en funci√≥n de las necesidades del usuario.

#### M√©tricas de Ranking Comunes

Para cuantificar la calidad de los resultados, se utilizan diversas m√©tricas. Algunas de las m√°s comunes incluyen:

- Precisi√≥n en k (P@k)
- Recall
- Mean Average Precision (MAP)
- Normalized Discounted Cumulative Gain (NDCG)
- Mean Reciprocal Rank (MRR)
- F1 Score
- Area Under the ROC curve (AUC-ROC)
- Mean Rank (MR)
- Hit Rate (HR) o Recall at K
- Expected Reciprocal Rank (ERR)

Cada una de estas m√©tricas captura distintos aspectos del rendimiento del sistema, como la precisi√≥n, el orden de los resultados o la cobertura.


### Entendiendo las M√©tricas

Evaluar un sistema de recuperaci√≥n o recomendaci√≥n no solo implica observar si los resultados son buenos, sino tambi√©n **cu√°n buenos**, **cu√°n r√°pidos** y **cu√°n completos** son. Para eso, se utilizan diversas m√©tricas, cada una con su propia l√≥gica y aplicaciones. A continuaci√≥n, exploramos las m√°s relevantes.

### **Precisi√≥n en k (P@k)**

Mide la proporci√≥n de documentos relevantes dentro de los primeros **k** resultados.

* **F√≥rmula:**  
  $P@k = \frac{\text{N√∫mero de documentos relevantes en los primeros k resultados}}{k}$

#### **Explicaci√≥n Detallada**

Esta es la m√©trica m√°s intuitiva. Imagina que buscas algo en Google y solo miras los primeros 10 resultados. La **Precisi√≥n en 10 (P@10)** te dice qu√© porcentaje de esos 10 enlaces fue realmente √∫til. No le importa si hab√≠a m√°s resultados buenos en la p√°gina 2; solo se enfoca en la calidad de la primera "vitrina" de resultados que ve el usuario.

#### **Casos de Uso Comunes**

- **Motores de b√∫squeda web (Google, Bing):** La mayor√≠a de los usuarios no pasa de la primera p√°gina.
- **B√∫squeda en e-commerce (Amazon):** Si buscas "zapatillas rojas", quieres ver zapatillas rojas inmediatamente.
- **Sistemas de recomendaci√≥n (Netflix, Spotify):** Es crucial que las primeras recomendaciones sean atractivas.

### **Recall (Exhaustividad o Sensibilidad)**

Mide la proporci√≥n de documentos relevantes que el sistema logr√≥ encontrar de entre el **total** de documentos relevantes que existen.

* **F√≥rmula:**  
  $Recall = \frac{\text{N√∫mero de documentos relevantes recuperados}}{\text{N√∫mero total de documentos relevantes}}$

#### **Explicaci√≥n Detallada**

El Recall se enfoca en la **cobertura**. No le importa si los resultados buenos est√°n al principio o al final, solo quiere saber si el sistema fue capaz de encontrarlos todos. El objetivo es minimizar los "falsos negativos", es decir, no dejar fuera nada importante.

#### **Casos de Uso Comunes**

- **B√∫squedas legales o de patentes:** Es cr√≠tico encontrar todos los documentos pertinentes.
- **Diagn√≥stico m√©dico:** Se requiere considerar todas las posibilidades.
- **Detecci√≥n de fraude o amenazas:** Es preferible detectar de m√°s (falsos positivos) que pasar por alto algo grave.

### **Puntuaci√≥n F1 (F1 Score)**

Es la media arm√≥nica de la Precisi√≥n y el Recall. Busca un equilibrio entre ambos.

* **F√≥rmula:**  
  $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$

#### **Explicaci√≥n Detallada**

La puntuaci√≥n F1 es el gran pacificador entre Precisi√≥n y Recall. Es imposible obtener un F1 alto si una de las dos m√©tricas es muy baja. La media arm√≥nica penaliza los extremos, obligando al sistema a ser balanceado.

#### **Casos de Uso Comunes**

- **Clasificaci√≥n de texto y an√°lisis de sentimiento.**
- **Evaluaci√≥n general de un sistema:** Proporciona una √∫nica cifra para resumir rendimiento.


### **Rango Rec√≠proco Medio (Mean Reciprocal Rank - MRR)**

Eval√∫a la posici√≥n en el ranking del **primer** documento relevante encontrado.

* **F√≥rmula:**  
  $MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$  
  - $|Q|$ es el n√∫mero total de b√∫squedas.  
  - $rank_i$ es la posici√≥n del primer resultado correcto para la b√∫squeda $i$.

#### **Explicaci√≥n Detallada**

¬øEl sistema encontr√≥ una buena respuesta r√°pido? Si la respuesta correcta aparece primero, el puntaje es 1. En la segunda posici√≥n, 1/2. En la tercera, 1/3, y as√≠ sucesivamente. El MRR promedia esta eficiencia a lo largo de muchas b√∫squedas.

#### **Casos de Uso Comunes**

- **Sistemas de preguntas y respuestas (FAQs).**
- **Chatbots de soporte.**
- **Autocompletado o resultados tipo "Voy a tener suerte".**

### **Ganancia Cumulativa Descontada Normalizada (NDCG)**

Mide la utilidad (o ganancia) de un documento seg√∫n su posici√≥n en la lista de resultados.

* **F√≥rmula:**  
  $NDCG = \frac{DCG}{IDCG}$  
  - **DCG:** suma la relevancia de cada documento, pero ‚Äúdescuenta‚Äù su valor cuanto m√°s abajo est√©.  
  - **IDCG:** es el DCG ideal (orden perfecto).

#### **Explicaci√≥n Detallada**

NDCG es una de las m√©tricas m√°s completas:
1. **Reconoce niveles de relevancia.**
2. **Penaliza posiciones bajas.**

El resultado normalizado var√≠a entre 0 y 1, donde 1 representa un ranking perfecto.

#### **Casos de Uso Comunes**

- **Motores de b√∫squeda y sistemas de recomendaci√≥n.**
- **Comparaci√≥n de modelos en competiciones y benchmarks.**

### **Precisi√≥n Media Promedio (Mean Average Precision - MAP)**

Calcula la precisi√≥n promedio para cada b√∫squeda y luego promedia estos valores entre todas.

* **F√≥rmula:**  
  $MAP = \frac{1}{|Q|} \sum_{q \in Q} \text{Average Precision(q)}$

#### **Explicaci√≥n Detallada**

El MAP recompensa que los documentos relevantes est√©n lo m√°s arriba posible en los resultados. Aunque no diferencia grados de relevancia, mide bien el orden general de los aciertos.

#### **Casos de Uso Comunes**

- **B√∫squeda de im√°genes o documentos.**
- **Benchmarks acad√©micos en recuperaci√≥n de informaci√≥n.**

### **Tasa de Aciertos (Hit Rate - HR)**

Mide la proporci√≥n de b√∫squedas donde se recuper√≥ **al menos un** documento relevante en el top **k**.

* **F√≥rmula:**  
  $HR@k = \frac{\text{N√∫mero de b√∫squedas con al menos un acierto en el top k}}{|Q|}$

#### **Explicaci√≥n Detallada**

Es una m√©trica de ‚Äútodo o nada‚Äù: ¬øhubo al menos un resultado √∫til en el top **k**? No se preocupa por cu√°ntos, solo por si hubo al menos uno.

#### **Casos de Uso Comunes**

- **Sistemas de recomendaci√≥n.**
- **Validaci√≥n de sistemas simples de recuperaci√≥n.**


### **Rango Rec√≠proco Esperado (Expected Reciprocal Rank - ERR)**

Eval√∫a la calidad del ranking considerando **la probabilidad de que un usuario encuentre una respuesta satisfactoria** en una posici√≥n determinada. A diferencia de MRR, tiene en cuenta m√∫ltiples niveles de relevancia y la probabilidad de que un usuario deje de buscar despu√©s de ver un resultado √∫til.

* **F√≥rmula (versi√≥n simplificada):**  
  $ERR = \sum_{r=1}^{n} \frac{1}{r} \cdot P(r)$  
  - $P(r)$ es la probabilidad de que el usuario se detenga en la posici√≥n $r$ (basada en la relevancia del documento en esa posici√≥n).

#### **Explicaci√≥n Detallada**

ERR modela un comportamiento **m√°s realista del usuario**: si encuentra un documento muy relevante en la primera posici√≥n, probablemente no siga mirando el resto. Pero si el documento es poco √∫til, seguir√° bajando.

Esta m√©trica se basa en la idea de que la relevancia no es binaria (s√≠/no), sino que puede tener diferentes niveles (por ejemplo: irrelevante, relevante, muy relevante). ERR combina esto con una penalizaci√≥n por posici√≥n, similar a NDCG, pero con una interpretaci√≥n probabil√≠stica.

#### **Casos de Uso Comunes**

- **Sistemas con m√∫ltiples grados de relevancia** como motores de b√∫squeda que clasifican resultados por utilidad.
- **Evaluaciones centradas en el usuario**, donde importa no solo qu√© tan relevante es un documento, sino tambi√©n cu√°ndo aparece en la lista.
- **Benchmarks avanzados**, como en competiciones de ranking (por ejemplo, TREC o LETOR).


## üõ†Ô∏è Ejemplo pr√°ctico: c√≥mo medir el rendimiento de tu sistema RAG

### Creaci√≥n de un entorno de desarrollo

Para crear y gestionar el entorno de Python de este proyecto se utiliza `uv`, una herramienta moderna que combina la gesti√≥n de entornos virtuales y la resoluci√≥n de dependencias de forma r√°pida y eficiente.

Este enfoque reemplaza el uso tradicional de herramientas como `venv`, `pip` y `virtualenv`, ofreciendo una experiencia m√°s simple y √°gil.

‚ÑπÔ∏è Para m√°s detalles sobre c√≥mo instalar y utilizar uv, consulta el archivo [`working-with-uv.md`](../docs/working-with-uv.md)

Una vez instalado `uv`, puedes crear el entorno virtual e instalar todas las dependencias necesarias con un solo comando:

```bash
uv venv && uv sync
```

Este comando crear√° un entorno virtual en el directorio del proyecto y sincronizar√° las librer√≠as especificadas en el archivo `pyproject.toml`.

### Evaluando la Recuperaci√≥n de Informaci√≥n en Sistemas RAG

#### **1. Generaci√≥n de datos de referencia (Ground Truth)**

Si deseas ver un ejemplo pr√°ctico de c√≥mo generar datos de referencia para evaluar un sistema RAG, puedes consultar el archivo [`ground-truth-data.ipynb`](./notebook/ground-truth-data.ipynb).

Este notebook incluye ejemplos de:
- Generaci√≥n de datos de referencia (ground truth) con ayuda de un LLM.
- Organizaci√≥n de las consultas y documentos relevantes.
- Preparaci√≥n de los datos para su posterior evaluaci√≥n.
- Opcionalmente, exportaci√≥n a un formato binario para su uso en experimentos posteriores.

Este paso es fundamental para contar con un conjunto s√≥lido de datos antes de realizar la indexaci√≥n.

#### **2. Evaluaci√≥n de respuestas generadas (Text Evaluation)**

Si deseas ver un ejemplo pr√°ctico de c√≥mo evaluar respuestas generadas por un sistema RAG, puedes consultar el archivo [`evaluate-text.ipynb`](./notebook/evaluate-text.ipynb).

Este notebook incluye ejemplos de:
- Evaluaci√≥n autom√°tica de respuestas usando m√©tricas como MRR (Mean Reciprocal Rank).
- Comparaci√≥n entre diferentes m√©todos para calcular la m√©trica.
- Revisi√≥n de casos con fallos y aciertos para entender el rendimiento del sistema.
- Preparaci√≥n de los resultados para visualizaci√≥n o an√°lisis posteriores.

Este paso es clave para medir la efectividad del sistema RAG a partir de los datos de referencia generados previamente.


## üîó Lectura recomendada
Recomendado para profundizar en los conceptos clave y ampliar tu comprensi√≥n
* [What is Qdrant?](https://qdrant.tech/documentation/overview/)


## ‚ñ∂Ô∏è Videos recomendados
Selecci√≥n de videos para reforzar visualmente los temas abordados
* [What is RAG? Building Better LLM Systems with Qdrant](https://www.youtube.com/watch?v=rtIyQPJUd_U)


## üìö Cursos adicionales recomendados
Recursos complementarios para seguir aprendiendo y fortaleciendo tus habilidades.

* [Retrieval Optimization: From Tokenization to Vector Quantization](https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization/?utm_campaign=qdrant-launch&utm_medium=qdrant&utm_source=partner-promo)

---

> üìå **Nota:** este repositorio complementa el curso **LLM Zoomcamp** de [DataTalks.Club](https://datatalks.club/), y contiene notas, lecturas, videos, ejemplos y recursos adicionales.  
> Para acceder al contenido oficial del curso, visita el [**repositorio principal en GitHub**](https://github.com/DataTalksClub/llm-zoomcamp).
