# Semana 3 - LLM Zoomcamp

Este documento recopila mis apuntes y recursos para la **Semana 3** del curso LLM Zoomcamp.

## ðŸ“ Notas de la teorÃ­a

### Importancia de la EvaluaciÃ³n

Es fundamental evaluar los resultados de bÃºsqueda para optimizar cÃ³mo se almacenan y recuperan los datos en un sistema RAG (Retrieval-Augmented Generation). Sin una evaluaciÃ³n adecuada, es difÃ­cil saber si el sistema estÃ¡ devolviendo resultados Ãºtiles y relevantes. El rendimiento ideal siempre depende de los datos disponibles y los requisitos especÃ­ficos del caso de uso.

El objetivo es medir de forma objetiva quÃ© tan bueno es un sistema de bÃºsqueda. En lugar de confiar en la intuiciÃ³n, usamos mÃ©tricas para cuantificar el rendimiento. Esto nos permite comparar diferentes sistemas (como Elasticsearch o Qdrant vs. otro motor de bÃºsqueda) o diferentes configuraciones del mismo sistema.


### Datos de Referencia (Ground Truth)

Para evaluar un sistema de recuperaciÃ³n, se necesita un conjunto de datos de referencia (tambiÃ©n conocido como *ground truth* o *gold standard data*). Este conjunto contiene consultas junto con los documentos relevantes para cada una.

#### Necesidad de un "Ground Truth"
Es indispensable contar con este conjunto para evaluar de manera efectiva el rendimiento del sistema. Cada entrada deberÃ­a vincular una pregunta con el documento que contiene la respuesta correcta.

#### Estructura del Conjunto de Datos
Idealmente, el conjunto contiene miles de preguntas, y para cada una se identifican los documentos relevantes. En algunos casos, como simplificaciÃ³n, se asume que existe **un solo documento relevante por pregunta**.

#### MÃ©todos para Crear Ground Truth

- **AnotaciÃ³n Humana**: Especialistas etiquetan manualmente las preguntas con sus documentos relevantes. Es el mÃ©todo mÃ¡s preciso, aunque costoso y lento.
- **ObservaciÃ³n del Usuario**: Se analiza el comportamiento real de los usuarios. Puede ser evaluado por humanos o por LLMs.
- **GeneraciÃ³n AutomÃ¡tica con LLM**: Usar un modelo como GPT-4 para generar preguntas y asociarlas automÃ¡ticamente a documentos de referencia (por ejemplo, desde un conjunto de preguntas frecuentes).

#### CreaciÃ³n de un ID de Documento Ãšnico
Para asegurar trazabilidad, se genera un hash (MD5) con los campos del documento (curso, pregunta, texto), lo que garantiza que el ID cambie solo si el contenido cambia.

#### Ejemplo de ImplementaciÃ³n
- Se carga un conjunto de documentos.
- Se asigna un ID Ãºnico a cada uno.
- Se diseÃ±a un prompt detallado que instruye al LLM a actuar como un estudiante y generar cinco preguntas variadas por FAQ.
- El resultado es almacenado como un DataFrame y luego exportado a CSV.

#### OptimizaciÃ³n de la BÃºsqueda

Varios parÃ¡metros de bÃºsqueda pueden ajustarse para mejorar los resultados obtenidos. Algunos ejemplos incluyen:

- Tipo de bÃºsqueda: densa, hÃ­brida, BM25.
- Campos del documento que se consultan: tÃ­tulo, cuerpo, metadatos, etc.
- Uso de boosting para priorizar campos como `question`.
- Filtros por categorÃ­a, curso o tipo de contenido.

La evaluaciÃ³n es la herramienta clave para identificar quÃ© configuraciÃ³n produce el mejor rendimiento en funciÃ³n de las necesidades del usuario.

#### MÃ©tricas de Ranking Comunes

Para cuantificar la calidad de los resultados, se utilizan diversas mÃ©tricas. Algunas de las mÃ¡s comunes incluyen:

- PrecisiÃ³n en k (P@k)
- Recall
- Mean Average Precision (MAP)
- Normalized Discounted Cumulative Gain (NDCG)
- Mean Reciprocal Rank (MRR)
- F1 Score
- Area Under the ROC curve (AUC-ROC)
- Mean Rank (MR)
- Hit Rate (HR) o Recall at K
- Expected Reciprocal Rank (ERR)

Cada una de estas mÃ©tricas captura distintos aspectos del rendimiento del sistema, como la precisiÃ³n, el orden de los resultados o la cobertura.


### Entendiendo las MÃ©tricas

Evaluar un sistema de recuperaciÃ³n o recomendaciÃ³n no solo implica observar si los resultados son buenos, sino tambiÃ©n **cuÃ¡n buenos**, **cuÃ¡n rÃ¡pidos** y **cuÃ¡n completos** son. Para eso, se utilizan diversas mÃ©tricas, cada una con su propia lÃ³gica y aplicaciones. A continuaciÃ³n, exploramos las mÃ¡s relevantes.

### **PrecisiÃ³n en k (P@k)**

Mide la proporciÃ³n de documentos relevantes dentro de los primeros **k** resultados.

* **FÃ³rmula:**  
  $P@k = \frac{\text{NÃºmero de documentos relevantes en los primeros k resultados}}{k}$

#### **ExplicaciÃ³n Detallada**

Esta es la mÃ©trica mÃ¡s intuitiva. Imagina que buscas algo en Google y solo miras los primeros 10 resultados. La **PrecisiÃ³n en 10 (P@10)** te dice quÃ© porcentaje de esos 10 enlaces fue realmente Ãºtil. No le importa si habÃ­a mÃ¡s resultados buenos en la pÃ¡gina 2; solo se enfoca en la calidad de la primera "vitrina" de resultados que ve el usuario.

#### **Casos de Uso Comunes**

- **Motores de bÃºsqueda web (Google, Bing):** La mayorÃ­a de los usuarios no pasa de la primera pÃ¡gina.
- **BÃºsqueda en e-commerce (Amazon):** Si buscas "zapatillas rojas", quieres ver zapatillas rojas inmediatamente.
- **Sistemas de recomendaciÃ³n (Netflix, Spotify):** Es crucial que las primeras recomendaciones sean atractivas.

### **Recall (Exhaustividad o Sensibilidad)**

Mide la proporciÃ³n de documentos relevantes que el sistema logrÃ³ encontrar de entre el **total** de documentos relevantes que existen.

* **FÃ³rmula:**  
  $Recall = \frac{\text{NÃºmero de documentos relevantes recuperados}}{\text{NÃºmero total de documentos relevantes}}$

#### **ExplicaciÃ³n Detallada**

El Recall se enfoca en la **cobertura**. No le importa si los resultados buenos estÃ¡n al principio o al final, solo quiere saber si el sistema fue capaz de encontrarlos todos. El objetivo es minimizar los "falsos negativos", es decir, no dejar fuera nada importante.

#### **Casos de Uso Comunes**

- **BÃºsquedas legales o de patentes:** Es crÃ­tico encontrar todos los documentos pertinentes.
- **DiagnÃ³stico mÃ©dico:** Se requiere considerar todas las posibilidades.
- **DetecciÃ³n de fraude o amenazas:** Es preferible detectar de mÃ¡s (falsos positivos) que pasar por alto algo grave.

### **PuntuaciÃ³n F1 (F1 Score)**

Es la media armÃ³nica de la PrecisiÃ³n y el Recall. Busca un equilibrio entre ambos.

* **FÃ³rmula:**  
  $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$

#### **ExplicaciÃ³n Detallada**

La puntuaciÃ³n F1 es el gran pacificador entre PrecisiÃ³n y Recall. Es imposible obtener un F1 alto si una de las dos mÃ©tricas es muy baja. La media armÃ³nica penaliza los extremos, obligando al sistema a ser balanceado.

#### **Casos de Uso Comunes**

- **ClasificaciÃ³n de texto y anÃ¡lisis de sentimiento.**
- **EvaluaciÃ³n general de un sistema:** Proporciona una Ãºnica cifra para resumir rendimiento.


### **Rango RecÃ­proco Medio (Mean Reciprocal Rank - MRR)**

EvalÃºa la posiciÃ³n en el ranking del **primer** documento relevante encontrado.

* **FÃ³rmula:**  
  $MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$  
  - $|Q|$ es el nÃºmero total de bÃºsquedas.  
  - $rank_i$ es la posiciÃ³n del primer resultado correcto para la bÃºsqueda $i$.

#### **ExplicaciÃ³n Detallada**

Â¿El sistema encontrÃ³ una buena respuesta rÃ¡pido? Si la respuesta correcta aparece primero, el puntaje es 1. En la segunda posiciÃ³n, 1/2. En la tercera, 1/3, y asÃ­ sucesivamente. El MRR promedia esta eficiencia a lo largo de muchas bÃºsquedas.

#### **Casos de Uso Comunes**

- **Sistemas de preguntas y respuestas (FAQs).**
- **Chatbots de soporte.**
- **Autocompletado o resultados tipo "Voy a tener suerte".**

### **Ganancia Cumulativa Descontada Normalizada (NDCG)**

Mide la utilidad (o ganancia) de un documento segÃºn su posiciÃ³n en la lista de resultados.

* **FÃ³rmula:**  
  $NDCG = \frac{DCG}{IDCG}$  
  - **DCG:** suma la relevancia de cada documento, pero â€œdescuentaâ€ su valor cuanto mÃ¡s abajo estÃ©.  
  - **IDCG:** es el DCG ideal (orden perfecto).

#### **ExplicaciÃ³n Detallada**

NDCG es una de las mÃ©tricas mÃ¡s completas:
1. **Reconoce niveles de relevancia.**
2. **Penaliza posiciones bajas.**

El resultado normalizado varÃ­a entre 0 y 1, donde 1 representa un ranking perfecto.

#### **Casos de Uso Comunes**

- **Motores de bÃºsqueda y sistemas de recomendaciÃ³n.**
- **ComparaciÃ³n de modelos en competiciones y benchmarks.**

### **PrecisiÃ³n Media Promedio (Mean Average Precision - MAP)**

Calcula la precisiÃ³n promedio para cada bÃºsqueda y luego promedia estos valores entre todas.

* **FÃ³rmula:**  
  $MAP = \frac{1}{|Q|} \sum_{q \in Q} \text{Average Precision(q)}$

#### **ExplicaciÃ³n Detallada**

El MAP recompensa que los documentos relevantes estÃ©n lo mÃ¡s arriba posible en los resultados. Aunque no diferencia grados de relevancia, mide bien el orden general de los aciertos.

#### **Casos de Uso Comunes**

- **BÃºsqueda de imÃ¡genes o documentos.**
- **Benchmarks acadÃ©micos en recuperaciÃ³n de informaciÃ³n.**

### **Tasa de Aciertos (Hit Rate - HR)**

Mide la proporciÃ³n de bÃºsquedas donde se recuperÃ³ **al menos un** documento relevante en el top **k**.

* **FÃ³rmula:**  
  $HR@k = \frac{\text{NÃºmero de bÃºsquedas con al menos un acierto en el top k}}{|Q|}$

#### **ExplicaciÃ³n Detallada**

Es una mÃ©trica de â€œtodo o nadaâ€: Â¿hubo al menos un resultado Ãºtil en el top **k**? No se preocupa por cuÃ¡ntos, solo por si hubo al menos uno.

#### **Casos de Uso Comunes**

- **Sistemas de recomendaciÃ³n.**
- **ValidaciÃ³n de sistemas simples de recuperaciÃ³n.**


### **Rango RecÃ­proco Esperado (Expected Reciprocal Rank - ERR)**

EvalÃºa la calidad del ranking considerando **la probabilidad de que un usuario encuentre una respuesta satisfactoria** en una posiciÃ³n determinada. A diferencia de MRR, tiene en cuenta mÃºltiples niveles de relevancia y la probabilidad de que un usuario deje de buscar despuÃ©s de ver un resultado Ãºtil.

* **FÃ³rmula (versiÃ³n simplificada):**  
  $ERR = \sum_{r=1}^{n} \frac{1}{r} \cdot P(r)$  
  - $P(r)$ es la probabilidad de que el usuario se detenga en la posiciÃ³n $r$ (basada en la relevancia del documento en esa posiciÃ³n).

#### **ExplicaciÃ³n Detallada**

ERR modela un comportamiento **mÃ¡s realista del usuario**: si encuentra un documento muy relevante en la primera posiciÃ³n, probablemente no siga mirando el resto. Pero si el documento es poco Ãºtil, seguirÃ¡ bajando.

Esta mÃ©trica se basa en la idea de que la relevancia no es binaria (sÃ­/no), sino que puede tener diferentes niveles (por ejemplo: irrelevante, relevante, muy relevante). ERR combina esto con una penalizaciÃ³n por posiciÃ³n, similar a NDCG, pero con una interpretaciÃ³n probabilÃ­stica.

#### **Casos de Uso Comunes**

- **Sistemas con mÃºltiples grados de relevancia** como motores de bÃºsqueda que clasifican resultados por utilidad.
- **Evaluaciones centradas en el usuario**, donde importa no solo quÃ© tan relevante es un documento, sino tambiÃ©n cuÃ¡ndo aparece en la lista.
- **Benchmarks avanzados**, como en competiciones de ranking (por ejemplo, TREC o LETOR).


## ðŸ› ï¸ Ejemplo prÃ¡ctico: cÃ³mo medir el rendimiento de tu sistema RAG

### CreaciÃ³n de un entorno de desarrollo

Para crear y gestionar el entorno de Python de este proyecto se utiliza `uv`, una herramienta moderna que combina la gestiÃ³n de entornos virtuales y la resoluciÃ³n de dependencias de forma rÃ¡pida y eficiente.

Este enfoque reemplaza el uso tradicional de herramientas como `venv`, `pip` y `virtualenv`, ofreciendo una experiencia mÃ¡s simple y Ã¡gil.

â„¹ï¸ Para mÃ¡s detalles sobre cÃ³mo instalar y utilizar uv, consulta el archivo [`working-with-uv.md`](../docs/working-with-uv.md)

Una vez instalado `uv`, puedes crear el entorno virtual e instalar todas las dependencias necesarias con un solo comando:

```bash
uv venv && uv sync
```

Este comando crearÃ¡ un entorno virtual en el directorio del proyecto y sincronizarÃ¡ las librerÃ­as especificadas en el archivo `pyproject.toml`.

### Evaluando la RecuperaciÃ³n de InformaciÃ³n en Sistemas RAG

#### **1. GeneraciÃ³n de datos de referencia (Ground Truth)**

Si deseas ver un ejemplo prÃ¡ctico de cÃ³mo generar datos de referencia para evaluar un sistema RAG, puedes consultar el archivo [`ground-truth-data.ipynb`](./notebook/ground-truth-data.ipynb).

Este notebook incluye ejemplos de:
- GeneraciÃ³n de datos de referencia (ground truth) con ayuda de un LLM.
- OrganizaciÃ³n de las consultas y documentos relevantes.
- PreparaciÃ³n de los datos para su posterior evaluaciÃ³n.
- Opcionalmente, exportaciÃ³n a un formato binario para su uso en experimentos posteriores.

Este paso es fundamental para contar con un conjunto sÃ³lido de datos antes de realizar la indexaciÃ³n.

#### **2. EvaluaciÃ³n de respuestas generadas (Text Evaluation)**

Si deseas ver un ejemplo prÃ¡ctico de cÃ³mo evaluar respuestas generadas por un sistema RAG, puedes consultar el archivo [`evaluate-text.ipynb`](./notebook/evaluate-text.ipynb).

Este notebook incluye ejemplos de:
- EvaluaciÃ³n automÃ¡tica de respuestas usando mÃ©tricas como MRR (Mean Reciprocal Rank).
- ComparaciÃ³n entre diferentes mÃ©todos para calcular la mÃ©trica.
- RevisiÃ³n de casos con fallos y aciertos para entender el rendimiento del sistema.
- PreparaciÃ³n de los resultados para visualizaciÃ³n o anÃ¡lisis posteriores.

Este paso es clave para medir la efectividad del sistema RAG a partir de los datos de referencia generados previamente.


## ðŸ”— Lectura recomendada
Recomendado para profundizar en los conceptos clave y ampliar tu comprensiÃ³n
* [What is Qdrant?](https://qdrant.tech/documentation/overview/)


## â–¶ï¸ Videos recomendados
SelecciÃ³n de videos para reforzar visualmente los temas abordados
* [What is RAG? Building Better LLM Systems with Qdrant](https://www.youtube.com/watch?v=rtIyQPJUd_U)


## ðŸ“š Cursos adicionales recomendados
Recursos complementarios para seguir aprendiendo y fortaleciendo tus habilidades.

* [Retrieval Optimization: From Tokenization to Vector Quantization](https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization/?utm_campaign=qdrant-launch&utm_medium=qdrant&utm_source=partner-promo)

---

> ðŸ“Œ **Nota:** este repositorio complementa el curso **LLM Zoomcamp** de [DataTalks.Club](https://datatalks.club/), y contiene notas, lecturas, videos, ejemplos y recursos adicionales.  
> Para acceder al contenido oficial del curso, visita el [**repositorio principal en GitHub**](https://github.com/DataTalksClub/llm-zoomcamp).
