# Semana 3 - LLM Zoomcamp

Este documento recopila mis apuntes y recursos para la **Semana 3** del curso LLM Zoomcamp.

## üìù Notas de la teor√≠a

### Importancia de la Evaluaci√≥n

Es fundamental evaluar los resultados de b√∫squeda para optimizar c√≥mo se almacenan y recuperan los datos en un sistema RAG (Retrieval-Augmented Generation). Sin una evaluaci√≥n adecuada, es dif√≠cil saber si el sistema est√° devolviendo resultados √∫tiles y relevantes. El rendimiento ideal siempre depende de los datos disponibles y los requisitos espec√≠ficos del caso de uso.

El objetivo es medir de forma objetiva qu√© tan bueno es un sistema de b√∫squeda. En lugar de confiar en la intuici√≥n, usamos m√©tricas para cuantificar el rendimiento. Esto nos permite comparar diferentes sistemas (como Elasticsearch o Qdrant vs. otro motor de b√∫squeda) o diferentes configuraciones del mismo sistema.


### Datos de Referencia (Ground Truth)

Para evaluar un sistema de recuperaci√≥n, se necesita un conjunto de datos de referencia (tambi√©n conocido como *ground truth* o *gold standard data*). Este conjunto contiene consultas junto con los documentos relevantes para cada una.

#### **Necesidad de un "Ground Truth"**
Es indispensable contar con este conjunto para evaluar de manera efectiva el rendimiento del sistema. Cada entrada deber√≠a vincular una pregunta con el documento que contiene la respuesta correcta.

#### **Estructura del Conjunto de Datos**
Idealmente, el conjunto contiene miles de preguntas, y para cada una se identifican los documentos relevantes. En algunos casos, como simplificaci√≥n, se asume que existe **un solo documento relevante por pregunta**.

#### **M√©todos para Crear Ground Truth**

- **Anotaci√≥n Humana**: Especialistas etiquetan manualmente las preguntas con sus documentos relevantes. Es el m√©todo m√°s preciso, aunque costoso y lento.
- **Observaci√≥n del Usuario**: Se analiza el comportamiento real de los usuarios. Puede ser evaluado por humanos o por LLMs.
- **Generaci√≥n Autom√°tica con LLM**: Usar un modelo como GPT-4 para generar preguntas y asociarlas autom√°ticamente a documentos de referencia (por ejemplo, desde un conjunto de preguntas frecuentes).

#### **Creaci√≥n de un ID de Documento √önico**
Para asegurar trazabilidad, se genera un hash (MD5) con los campos del documento (curso, pregunta, texto), lo que garantiza que el ID cambie solo si el contenido cambia.

#### **Ejemplo de Implementaci√≥n**
- Se carga un conjunto de documentos.
- Se asigna un ID √∫nico a cada uno.
- Se dise√±a un prompt detallado que instruye al LLM a actuar como un estudiante y generar cinco preguntas variadas por FAQ.
- El resultado es almacenado como un DataFrame y luego exportado a CSV.

#### **Optimizaci√≥n de la B√∫squeda**

Varios par√°metros de b√∫squeda pueden ajustarse para mejorar los resultados obtenidos. Algunos ejemplos incluyen:

- Tipo de b√∫squeda: densa, h√≠brida, BM25.
- Campos del documento que se consultan: t√≠tulo, cuerpo, metadatos, etc.
- Uso de boosting para priorizar campos como `question`.
- Filtros por categor√≠a, curso o tipo de contenido.

La evaluaci√≥n es la herramienta clave para identificar qu√© configuraci√≥n produce el mejor rendimiento en funci√≥n de las necesidades del usuario.

#### **M√©tricas de Ranking Comunes**

Para cuantificar la calidad de los resultados, se utilizan diversas m√©tricas. Algunas de las m√°s comunes incluyen:

- Precisi√≥n en k (P@k)
- Recall
- Mean Average Precision (MAP)
- Normalized Discounted Cumulative Gain (NDCG)
- Mean Reciprocal Rank (MRR)
- F1 Score
- Area Under the ROC curve (AUC-ROC)
- Mean Rank (MR)
- Hit Rate (HR) o Recall at K
- Expected Reciprocal Rank (ERR)

Cada una de estas m√©tricas captura distintos aspectos del rendimiento del sistema, como la precisi√≥n, el orden de los resultados o la cobertura.

### Evaluaci√≥n y Monitoreo

Aunque los sistemas RAG utilizan modelos de lenguaje como parte del proceso, los LLMs tambi√©n pueden usarse **de manera independiente** en tareas como generaci√≥n de texto, clasificaci√≥n o razonamiento. Por eso, es importante entender c√≥mo evaluarlos y monitorearlos en distintos contextos.

#### **Evaluaci√≥n de LLMs: ¬øQu√© se mide y c√≥mo?**

- **Calidad del contenido generado**  
  Se busca responder: ¬øes √∫til, coherente, relevante y veraz lo que produce el modelo? Algunas formas comunes de evaluar esto son:

  - **Human Evaluation**  
    Personas humanas califican las respuestas del modelo en dimensiones como claridad, utilidad, precisi√≥n factual o tono.

  - **Automatic Metrics**  
    Herramientas que cuantifican la calidad de las respuestas:
    - `BLEU`, `ROUGE`, `METEOR`: comparan respuestas generadas con referencias.
    - `BERTScore`: mide similitud sem√°ntica entre textos.
    - **Similitud Embedding (coseno)**: se representa el texto en vectores y se mide su cercan√≠a matem√°tica.

  - **LLM-as-a-Judge**  
    Otro LLM act√∫a como evaluador. Se le da una instrucci√≥n del tipo:
    "Dada esta pregunta y esta respuesta generada, ¬øqu√© tan √∫til y correcta es la respuesta?"

  - **Pruebas A/B**  
    Se comparan dos versiones del modelo (por ejemplo, una con un nuevo prompt o ajuste de hiperpar√°metros) con usuarios reales. A cada grupo se le muestra una versi√≥n diferente, y se recopilan m√©tricas como satisfacci√≥n, tasa de clics o preferencia expl√≠cita entre respuestas.


- **Evaluaci√≥n de comportamiento no deseado**  
  Los LLMs tambi√©n se eval√∫an para detectar errores como:
  - **Alucinaciones**: respuestas que suenan correctas pero son falsas.
  - **Bias o toxicidad**: lenguaje ofensivo o sesgado.
  - **Seguridad**: si el modelo responde a preguntas da√±inas o peligrosas.

#### **Monitoreo de LLMs en Producci√≥n**

Una vez que el LLM est√° en uso (en apps, asistentes, bots, etc.), el monitoreo continuo es clave para mantener su calidad y seguridad.

- **M√©tricas t√©cnicas**  
  - Latencia de respuesta  
  - Tasa de error o ca√≠das del sistema  
  - Costo por consulta (tokens, tiempo de c√≥mputo)

- **M√©tricas de uso y comportamiento**  
  - N√∫mero de usuarios activos  
  - Tasa de √©xito en tareas (ejemplo: generaci√≥n correcta de reportes, respuestas √∫tiles)  
  - Detecci√≥n de respuestas problem√°ticas o alucinaciones en tiempo real

- **Retroalimentaci√≥n del usuario**  
  - Pulgar arriba/abajo  
  - Comentarios abiertos  
  - NPS (Net Promoter Score) sobre la experiencia con el modelo

#### **Importancia de la Evaluaci√≥n y el Monitoreo Combinados**

Un buen LLM no solo debe pasar pruebas antes de salir a producci√≥n, sino tambi√©n demostrar que **mantiene su rendimiento bajo condiciones reales** y con usuarios diversos. La combinaci√≥n de evaluaci√≥n sistem√°tica y monitoreo activo permite:

- Detectar degradaciones en la calidad.
- Responder r√°pidamente a incidentes de seguridad.
- Identificar oportunidades para mejorar el modelo o ajustar prompts.

Evaluar y monitorear LLMs es un proceso continuo, que combina **pruebas offline** (en entornos controlados antes del despliegue, como benchmarks o datasets con ground truth), **pruebas online** (con usuarios reales y m√©tricas en tiempo real), **recolecci√≥n de datos de uso** y **herramientas autom√°ticas** (como evaluadores autom√°ticos o LLM-as-a-Judge), para garantizar calidad, seguridad y valor para el usuario final.

#### **Evaluaci√≥n Offline de Sistemas RAG**

La **evaluaci√≥n offline** en sistemas de Recuperaci√≥n Aumentada por Generaci√≥n (RAG) es esencial para validar la **calidad** de las respuestas **antes del despliegue en producci√≥n**. Este proceso permite comparar modelos, ajustar configuraciones y optimizar el balance entre **precisi√≥n, velocidad y costo**.

**Componentes de un sistema RAG**

Un sistema RAG consta de tres componentes clave:

1. **B√∫squeda (Retrieval):**  
   Recupera documentos relevantes utilizando b√∫squedas sem√°nticas en bases vectoriales como **Qdrant**, **Elasticsearch** o **FAISS**.

2. **Construcci√≥n del prompt:**  
   Ensambla la consulta del usuario con los documentos recuperados, generando una entrada coherente para el modelo.

3. **Generaci√≥n con LLM:**  
   El modelo de lenguaje procesa el prompt y genera una respuesta final basada en el contexto aportado.

**Proceso de evaluaci√≥n**

La evaluaci√≥n offline simula interacciones antes de poner el sistema frente a usuarios reales. Utiliza **datasets de referencia** para medir el desempe√±o con m√©tricas cuantitativas. A continuaci√≥n se explica un proceso paso a paso:

1. **Indexaci√≥n de datos:**  
   Los documentos son convertidos en vectores con modelos como `multi-qa-MiniLM-L6-cos-v1`.

2. **Generaci√≥n de preguntas sint√©ticas (opcional):**  
   A partir de las respuestas ideales (ground truth), un LLM genera una pregunta coherente.

3. **Obtenci√≥n de respuesta generada:**  
   Se eval√∫a c√≥mo responde el modelo cuando recibe esa pregunta.

4. **Comparaci√≥n entre respuestas (A vs A‚Äô):**  
   Se mide cu√°n parecida es la respuesta generada (A') a la ideal (A), usando m√©tricas como:

   - **Similitud del coseno:**  
     Eval√∫a la cercan√≠a sem√°ntica entre vectores de texto.  
     - `1.0`: muy similares  
     - `0.0`: completamente diferentes

   - **Otras m√©tricas complementarias:**  
     - `Exact Match`  
     - `F1 Score`  
     - `BLEU`, `ROUGE` (√∫tiles cuando no se trabaja con embeddings)

**Comparativa de modelos**

La elecci√≥n del modelo LLM no debe hacerse de forma arbitraria. Existen m√∫ltiples opciones disponibles, y cada una presenta ventajas y desventajas en t√©rminos de **costo**, **tiempo de procesamiento** y **calidad de resultados**. Evaluar estos factores permite seleccionar el modelo m√°s adecuado seg√∫n el contexto y los objetivos del sistema RAG.

**Caso Pr√°ctico: Comparaci√≥n de Modelos LLM de OpenAI**

Se compararon tres modelos populares para ver cu√°l ofrece mejor relaci√≥n **calidad / costo / tiempo**:

| Modelo         | Similitud promedio | Costo aprox. por lote | Tiempo de procesamiento |
|----------------|--------------------|------------------------|--------------------------|
| **GPT-4o Mini** | 0.683              | $0.10                  | 6.5 minutos (con multithreading) |
| **GPT-4**       | 0.679              | $10.00                 | 3 horas                  |
| **GPT-3.5 Turbo**| 0.657              | $0.79                  | 6.5 minutos              |

**Observaciones claves:**

- Las puntuaciones de calidad son **muy similares**.
- **GPT-4o Mini** destaca por ser **m√°s r√°pido y econ√≥mico**, aunque con restricciones de **rate limit**.
- Con t√©cnicas como `ThreadPoolExecutor` se puede acelerar el proceso sin comprometer calidad.

**Visualizaci√≥n y M√©tricas**

Se recomienda visualizar los resultados con herramientas como:

- `sns.histplot()` para histogramas de similitud
- `sns.kdeplot()` para estimaciones de densidad
- `sns.ecdfplot()` para distribuci√≥n acumulada

**Herramientas √∫tiles**

- **RAGAS**: Eval√∫a m√∫ltiples dimensiones (fidelidad, relevancia, recuerdo)
- **LangChain Evaluation**: Framework flexible para comparar cadenas y prompts
- **Pandas**: Gesti√≥n estructurada de resultados
- **tqdm**: Visualizaci√≥n del progreso
- **PromptLayer** y **Evidently**: Para trazabilidad y an√°lisis en producci√≥n

La evaluaci√≥n offline no solo permite elegir el mejor modelo, sino tambi√©n tomar decisiones informadas que optimicen el rendimiento general del sistema RAG.

> **Evaluar no es un paso √∫nico, es un proceso continuo.**

#### **LLM como Juez en la Evaluaci√≥n Offline y Online**

Cuando no se dispone de una respuesta ideal (ground truth), o se busca un enfoque m√°s cualitativo, puede emplearse la t√©cnica **LLM-as-a-Judge**, donde otro modelo LLM eval√∫a las respuestas generadas.

**Escenario 1: Evaluaci√≥n Offline (con ground truth)**

- **Contexto:** Evaluaci√≥n previa al despliegue, en entorno controlado.
- **Datos disponibles para el LLM juez:**
  - Pregunta del usuario.
  - Respuesta generada por el sistema.
  - Respuesta ideal (ground truth).
- **Tarea del juez:** Clasificar la respuesta generada como `RELEVANTE`, `PARCIALMENTE RELEVANTE` o `NO RELEVANTE`, explicando su razonamiento.

**Escenario 2: Evaluaci√≥n Online (sin ground truth)**

- **Contexto:** Evaluaci√≥n en producci√≥n, con datos en tiempo real.
- **Datos disponibles para el LLM juez:**
  - Pregunta del usuario.
  - Respuesta generada por el sistema.
- **Tarea del juez:** Evaluar si la respuesta es coherente y √∫til, sin referencia directa. Ideal para monitoreo continuo o testing en vivo.

**Ventaja:** Esta t√©cnica permite realizar evaluaciones m√°s subjetivas o de criterio experto, incluso cuando no se dispone de respuestas ideales.

**Dise√±o de Prompts para el Juez LLM**

Para asegurar una evaluaci√≥n consistente y √∫til, el LLM juez necesita instrucciones claras. Se recomienda usar prompts como:

- **Prompt para Evaluaci√≥n Offline:**  
  `"Analiza la respuesta generada en relaci√≥n con la respuesta original. Clasif√≠cala como RELEVANTE, PARCIALMENTE RELEVANTE o NO RELEVANTE. Justifica tu elecci√≥n."`

- **Prompt para Evaluaci√≥n Online:**  
  `"Eval√∫a si la respuesta proporcionada es relevante para la pregunta. Clasif√≠cala como RELEVANTE, PARCIALMENTE RELEVANTE o NO RELEVANTE. Justifica tu decisi√≥n."`

**Consejo t√©cnico:** Estructura las respuestas del juez en formato **JSON** para facilitar su procesamiento autom√°tico.

**Lecciones Clave y Consideraciones T√©cnicas**

- **Las respuestas incorrectas son oro:** Las clasificaciones como "NO RELEVANTE" ayudan a detectar fallos cr√≠ticos en el sistema RAG (por ejemplo, recuperaci√≥n err√≥nea o prompts mal construidos).
- **Identificaci√≥n de cuellos de botella:** El an√°lisis cualitativo permite detectar si el problema radica en la b√∫squeda, construcci√≥n del prompt o generaci√≥n del LLM.
- **Monitoreo continuo:** En entornos productivos, el LLM juez permite mantener control de calidad sin intervenci√≥n humana directa.

### Entendiendo las M√©tricas

Evaluar un sistema de recuperaci√≥n o recomendaci√≥n no solo implica observar si los resultados son buenos, sino tambi√©n **cu√°n buenos**, **cu√°n r√°pidos** y **cu√°n completos** son. Para eso, se utilizan diversas m√©tricas, cada una con su propia l√≥gica y aplicaciones. A continuaci√≥n, exploramos las m√°s relevantes.

#### Precisi√≥n en k (P@k)

Mide la proporci√≥n de documentos relevantes dentro de los primeros **k** resultados.

* **F√≥rmula:**  
  $P@k = \frac{\text{N√∫mero de documentos relevantes en los primeros k resultados}}{k}$

**Explicaci√≥n Detallada**

Esta es la m√©trica m√°s intuitiva. Imagina que buscas algo en Google y solo miras los primeros 10 resultados. La **Precisi√≥n en 10 (P@10)** te dice qu√© porcentaje de esos 10 enlaces fue realmente √∫til. No le importa si hab√≠a m√°s resultados buenos en la p√°gina 2; solo se enfoca en la calidad de la primera "vitrina" de resultados que ve el usuario.

**Casos de Uso Comunes**

- **Motores de b√∫squeda web (Google, Bing):** La mayor√≠a de los usuarios no pasa de la primera p√°gina.
- **B√∫squeda en e-commerce (Amazon):** Si buscas "zapatillas rojas", quieres ver zapatillas rojas inmediatamente.
- **Sistemas de recomendaci√≥n (Netflix, Spotify):** Es crucial que las primeras recomendaciones sean atractivas.

#### Recall (Exhaustividad o Sensibilidad)

Mide la proporci√≥n de documentos relevantes que el sistema logr√≥ encontrar de entre el **total** de documentos relevantes que existen.

* **F√≥rmula:**  
  $Recall = \frac{\text{N√∫mero de documentos relevantes recuperados}}{\text{N√∫mero total de documentos relevantes}}$

**Explicaci√≥n Detallada**

El Recall se enfoca en la **cobertura**. No le importa si los resultados buenos est√°n al principio o al final, solo quiere saber si el sistema fue capaz de encontrarlos todos. El objetivo es minimizar los "falsos negativos", es decir, no dejar fuera nada importante.

**Casos de Uso Comunes**

- **B√∫squedas legales o de patentes:** Es cr√≠tico encontrar todos los documentos pertinentes.
- **Diagn√≥stico m√©dico:** Se requiere considerar todas las posibilidades.
- **Detecci√≥n de fraude o amenazas:** Es preferible detectar de m√°s (falsos positivos) que pasar por alto algo grave.

#### Puntuaci√≥n F1 (F1 Score)

Es la media arm√≥nica de la Precisi√≥n y el Recall. Busca un equilibrio entre ambos.

* **F√≥rmula:**  
  $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$

**Explicaci√≥n Detallada**

La puntuaci√≥n F1 es el gran pacificador entre Precisi√≥n y Recall. Es imposible obtener un F1 alto si una de las dos m√©tricas es muy baja. La media arm√≥nica penaliza los extremos, obligando al sistema a ser balanceado.

**Casos de Uso Comunes**

- **Clasificaci√≥n de texto y an√°lisis de sentimiento.**
- **Evaluaci√≥n general de un sistema:** Proporciona una √∫nica cifra para resumir rendimiento.


#### Rango Rec√≠proco Medio (Mean Reciprocal Rank - MRR)

Eval√∫a la posici√≥n en el ranking del **primer** documento relevante encontrado.

* **F√≥rmula:**  
  $MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$  
  - $|Q|$ es el n√∫mero total de b√∫squedas.  
  - $rank_i$ es la posici√≥n del primer resultado correcto para la b√∫squeda $i$.

**Explicaci√≥n Detallada**

¬øEl sistema encontr√≥ una buena respuesta r√°pido? Si la respuesta correcta aparece primero, el puntaje es 1. En la segunda posici√≥n, 1/2. En la tercera, 1/3, y as√≠ sucesivamente. El MRR promedia esta eficiencia a lo largo de muchas b√∫squedas.

**Casos de Uso Comunes**

- **Sistemas de preguntas y respuestas (FAQs).**
- **Chatbots de soporte.**
- **Autocompletado o resultados tipo "Voy a tener suerte".**

#### Ganancia Cumulativa Descontada Normalizada (NDCG)

Mide la utilidad (o ganancia) de un documento seg√∫n su posici√≥n en la lista de resultados.

* **F√≥rmula:**  
  $NDCG = \frac{DCG}{IDCG}$  
  - **DCG:** suma la relevancia de cada documento, pero ‚Äúdescuenta‚Äù su valor cuanto m√°s abajo est√©.  
  - **IDCG:** es el DCG ideal (orden perfecto).

**Explicaci√≥n Detallada**

NDCG es una de las m√©tricas m√°s completas:
1. **Reconoce niveles de relevancia.**
2. **Penaliza posiciones bajas.**

El resultado normalizado var√≠a entre 0 y 1, donde 1 representa un ranking perfecto.

**Casos de Uso Comunes**

- **Motores de b√∫squeda y sistemas de recomendaci√≥n.**
- **Comparaci√≥n de modelos en competiciones y benchmarks.**

#### Precisi√≥n Media Promedio (Mean Average Precision - MAP)

Calcula la precisi√≥n promedio para cada b√∫squeda y luego promedia estos valores entre todas.

* **F√≥rmula:**  
  $MAP = \frac{1}{|Q|} \sum_{q \in Q} \text{Average Precision(q)}$

**Explicaci√≥n Detallada**

El MAP recompensa que los documentos relevantes est√©n lo m√°s arriba posible en los resultados. Aunque no diferencia grados de relevancia, mide bien el orden general de los aciertos.

**Casos de Uso Comunes**

- **B√∫squeda de im√°genes o documentos.**
- **Benchmarks acad√©micos en recuperaci√≥n de informaci√≥n.**

#### Tasa de Aciertos (Hit Rate - HR)

Mide la proporci√≥n de b√∫squedas donde se recuper√≥ **al menos un** documento relevante en el top **k**.

* **F√≥rmula:**  
  $HR@k = \frac{\text{N√∫mero de b√∫squedas con al menos un acierto en el top k}}{|Q|}$

**Explicaci√≥n Detallada**

Es una m√©trica de ‚Äútodo o nada‚Äù: ¬øhubo al menos un resultado √∫til en el top **k**? No se preocupa por cu√°ntos, solo por si hubo al menos uno.

**Casos de Uso Comunes**

- **Sistemas de recomendaci√≥n.**
- **Validaci√≥n de sistemas simples de recuperaci√≥n.**


#### Rango Rec√≠proco Esperado (Expected Reciprocal Rank - ERR)

Eval√∫a la calidad del ranking considerando **la probabilidad de que un usuario encuentre una respuesta satisfactoria** en una posici√≥n determinada. A diferencia de MRR, tiene en cuenta m√∫ltiples niveles de relevancia y la probabilidad de que un usuario deje de buscar despu√©s de ver un resultado √∫til.

* **F√≥rmula (versi√≥n simplificada):**  
  $ERR = \sum_{r=1}^{n} \frac{1}{r} \cdot P(r)$  
  - $P(r)$ es la probabilidad de que el usuario se detenga en la posici√≥n $r$ (basada en la relevancia del documento en esa posici√≥n).

**Explicaci√≥n Detallada**

ERR modela un comportamiento **m√°s realista del usuario**: si encuentra un documento muy relevante en la primera posici√≥n, probablemente no siga mirando el resto. Pero si el documento es poco √∫til, seguir√° bajando.

Esta m√©trica se basa en la idea de que la relevancia no es binaria (s√≠/no), sino que puede tener diferentes niveles (por ejemplo: irrelevante, relevante, muy relevante). ERR combina esto con una penalizaci√≥n por posici√≥n, similar a NDCG, pero con una interpretaci√≥n probabil√≠stica.

**Casos de Uso Comunes**

- **Sistemas con m√∫ltiples grados de relevancia** como motores de b√∫squeda que clasifican resultados por utilidad.
- **Evaluaciones centradas en el usuario**, donde importa no solo qu√© tan relevante es un documento, sino tambi√©n cu√°ndo aparece en la lista.
- **Benchmarks avanzados**, como en competiciones de ranking (por ejemplo, TREC o LETOR).

---

Cuando se quiere evaluar autom√°ticamente la calidad de textos generados por un modelo, es com√∫n comparar esos textos con respuestas de referencia utilizando m√©tricas espec√≠ficas. A continuaci√≥n, se explican algunas de las m√°s utilizadas.

#### BLEU (Bilingual Evaluation Understudy)

Eval√∫a la calidad de un texto generado comparando **n-gramas coincidentes** (secuencias de palabras) con una o m√°s referencias.

* **Idea clave:** Si el modelo usa muchas frases similares a las de la referencia, la calidad es alta.
* **Penalizaci√≥n por longitud:** BLEU incluye una penalizaci√≥n si el texto generado es mucho m√°s corto que la referencia.

**F√≥rmula (simplificada):**  
`BLEU = BP √ó exp(‚àë w‚Çô √ó log(p‚Çô))`  
- `BP`: *brevity penalty*  
- `p‚Çô`: precisi√≥n de n-gramas de orden `n`  
- `w‚Çô`: peso asignado a cada orden (por ejemplo, 0.25 si se usan 4-gramas)

**Casos de Uso Comunes**  
- Traducci√≥n autom√°tica  
- Generaci√≥n de texto donde se espera que el output se acerque mucho a una referencia  

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

Mide la **superposici√≥n de n-gramas o frases** entre la salida del modelo y una o varias referencias.

* **Variantes comunes:**
  - `ROUGE-1`: superposici√≥n de palabras individuales
  - `ROUGE-2`: superposici√≥n de bigramas (pares de palabras)
  - `ROUGE-L`: longitud de la subsecuencia com√∫n m√°s larga (*Longest Common Subsequence*)

**Enfoque en recall:** A diferencia de BLEU (que prioriza precisi√≥n), ROUGE est√° orientado a capturar cu√°nta informaci√≥n relevante se ha recuperado.

**Casos de Uso Comunes**  
- Evaluaci√≥n de res√∫menes autom√°ticos  
- Tareas donde es m√°s importante **capturar lo esencial** del contenido  

#### METEOR (Metric for Evaluation of Translation with Explicit ORdering)

Mejora algunas limitaciones de BLEU y ROUGE utilizando:
- Coincidencias por sin√≥nimos y variantes morfol√≥gicas
- Penalizaciones por orden incorrecto
- Evaluaci√≥n a nivel de frase

**Ventajas:**  
- Mayor correlaci√≥n con evaluaciones humanas  
- Soporta flexibilidad ling√º√≠stica  

**Casos de Uso Comunes**  
- Traducci√≥n autom√°tica  
- Generaci√≥n de lenguaje natural en contextos sensibles a la sem√°ntica  

#### BERTScore

Eval√∫a la **similitud sem√°ntica** entre el texto generado y la referencia utilizando **representaciones vectoriales obtenidas con BERT** (o modelos similares).

* **C√≥mo funciona:**
  - Convierte cada palabra en un vector (*embedding*)
  - Compara los vectores del texto generado con los de la referencia
  - Usa m√©tricas de similitud como el coseno

**Ventajas:**  
- Capta mejor el *significado*, no solo la coincidencia superficial de palabras  
- Funciona bien en tareas donde el lenguaje puede ser diverso pero el mensaje debe mantenerse  

**Casos de Uso Comunes**  
- Resumen autom√°tico  
- Parafraseo  
- Generaci√≥n abierta de texto donde puede haber muchas respuestas correctas  

Estas m√©tricas ofrecen una primera aproximaci√≥n r√°pida y cuantitativa a la calidad del texto generado. Sin embargo, en muchos casos **es necesario complementarlas con evaluaciones humanas o LLM-as-a-Judge** para obtener una visi√≥n completa.



## üõ†Ô∏è Ejemplo pr√°ctico: c√≥mo medir el rendimiento de tu sistema RAG

### Creaci√≥n de un entorno de desarrollo

Para crear y gestionar el entorno de Python de este proyecto se utiliza `uv`, una herramienta moderna que combina la gesti√≥n de entornos virtuales y la resoluci√≥n de dependencias de forma r√°pida y eficiente.

Este enfoque reemplaza el uso tradicional de herramientas como `venv`, `pip` y `virtualenv`, ofreciendo una experiencia m√°s simple y √°gil.

‚ÑπÔ∏è Para m√°s detalles sobre c√≥mo instalar y utilizar uv, consulta el archivo [`working-with-uv.md`](../docs/working-with-uv.md)

Una vez instalado `uv`, puedes crear el entorno virtual e instalar todas las dependencias necesarias con un solo comando:

```bash
uv venv && uv sync
```

Este comando crear√° un entorno virtual en el directorio del proyecto y sincronizar√° las librer√≠as especificadas en el archivo `pyproject.toml`.

### Evaluando la Recuperaci√≥n de Informaci√≥n en Sistemas RAG

#### **1. Generaci√≥n de datos de referencia (Ground Truth)**

Si deseas ver un ejemplo pr√°ctico de c√≥mo generar datos de referencia para evaluar un sistema RAG, puedes consultar el archivo [`ground-truth-data.ipynb`](./notebook/ground-truth-data.ipynb).

Este notebook incluye ejemplos de:
- Generaci√≥n de datos de referencia (ground truth) con ayuda de un LLM.
- Organizaci√≥n de las consultas y documentos relevantes.
- Preparaci√≥n de los datos para su posterior evaluaci√≥n.
- Opcionalmente, exportaci√≥n a un formato binario para su uso en experimentos posteriores.

Este paso es fundamental para contar con un conjunto s√≥lido de datos antes de realizar la indexaci√≥n.

#### **2. Evaluaci√≥n de respuestas generadas (Text Evaluation)**

Si deseas ver un ejemplo pr√°ctico de c√≥mo evaluar respuestas generadas por un sistema RAG, puedes consultar el archivo [`evaluate-text.ipynb`](./notebook/evaluate-text.ipynb).

Este notebook incluye ejemplos de:
- Evaluaci√≥n autom√°tica de respuestas usando m√©tricas como MRR (Mean Reciprocal Rank).
- Comparaci√≥n entre diferentes m√©todos para calcular la m√©trica.
- Revisi√≥n de casos con fallos y aciertos para entender el rendimiento del sistema.
- Preparaci√≥n de los resultados para visualizaci√≥n o an√°lisis posteriores.

Este paso es clave para medir la efectividad del sistema RAG a partir de los datos de referencia generados previamente.


## üîó Lectura recomendada
Recomendado para profundizar en los conceptos clave y ampliar tu comprensi√≥n

* [Evaluating RAG Architectures on Benchmark Tasks](https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/comparing_techniques.html)
* [Best Practices for LLM Evaluation of RAG Applications](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)
* [Awesome RAG Evaluation](https://github.com/YHPeter/Awesome-RAG-Evaluation?tab=readme-ov-file)
* [An Overview on RAG Evaluation](https://weaviate.io/blog/rag-evaluation)
* [La gu√≠a definitiva para evaluar los componentes del sistema RAG: lo que necesitas saber](https://myscale.com/blog/es/ultimate-guide-to-evaluate-rag-system/)
* [Evaluadores de generaci√≥n aumentada por recuperaci√≥n (RAG)](https://learn.microsoft.com/es-es/azure/ai-foundry/concepts/evaluation-evaluators/rag-evaluators)
* [Let's talk about LLM evaluation](https://huggingface.co/blog/clefourrier/llm-evaluation)
* [RAG Evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation)
* [RAG Evaluation: Don‚Äôt let customers tell you first](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/)
* [LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#:~:text=1,is%20able%20to%20call%20the)
* [Evaluating model performance](https://platform.openai.com/docs/guides/evals)
* [Evaluaci√≥n de LLMs: Principales benchmarks y c√≥mo entenderlos](https://www.nerds.ai/blog/evaluacion-de-llms-principales-benchmarks-y-como-entenderlos)
* [Ragas core Concepts](https://docs.ragas.io/en/stable/concepts/)
* [SentenceTransformers Documentation](https://sbert.net/)
* [SentenceTransformers Quickstart](https://sbert.net/docs/quickstart.html)
* [SentenceTransformers Pretrained ModelsÔÉÅ](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)
* [LLM evaluation: a beginner's guide](https://www.evidentlyai.com/llm-guide/llm-evaluation)
* [La Arquitectura de la Confianza: Una Gu√≠a para la Evaluaci√≥n de Sistemas de IA, LLM y RAG](https://medium.com/@j92riquelme/la-arquitectura-de-la-confianza-una-guia-para-la-evaluacion-de-sistemas-de-ia-be5be4aed808)


## ‚ñ∂Ô∏è Videos recomendados
Selecci√≥n de videos para reforzar visualmente los temas abordados

* [LLM evaluation for builders - Course announcement](https://www.youtube.com/watch?v=jQgI8tTkWQU&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d)
* [Welcome to the LLM evaluation course](https://www.youtube.com/watch?v=rHs0sP7b5fM&list=PL9omX6impEuMgDFCK_NleIB0sMzKs2boI)


## üìö Cursos adicionales recomendados
Recursos complementarios para seguir aprendiendo y fortaleciendo tus habilidades.

* [Ragas tutorials](https://docs.ragas.io/en/stable/getstarted/)
* [LLM apps: Evaluation](https://wandb.ai/site/courses/evals/?utm_source=chatgpt.com)
* [Building and Evaluating Advanced RAG Applications](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?utm_source=chatgpt.com)
* [Evaluaci√≥n de modelos de lenguaje con Azure Databricks](https://learn.microsoft.com/es-es/training/modules/evaluate-language-models-azure-databricks/?utm_source=chatgpt.com)

---

> üìå **Nota:** este repositorio complementa el curso **LLM Zoomcamp** de [DataTalks.Club](https://datatalks.club/), y contiene notas, lecturas, videos, ejemplos y recursos adicionales.  
> Para acceder al contenido oficial del curso, visita el [**repositorio principal en GitHub**](https://github.com/DataTalksClub/llm-zoomcamp).
