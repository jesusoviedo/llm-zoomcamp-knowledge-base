
### 🎥 **LLM Zoomcamp 3.0: Novedades rumbo a la edición 2025**

**Reestructuración de Módulos:**  
Se eliminó el módulo de código abierto y el módulo de búsqueda vectorial se movió a la segunda posición.

**Nuevo Módulo de Evaluación:**  
La sección de "evaluación de recuperación", que antes formaba parte del módulo de búsqueda vectorial, ahora es un módulo independiente (Módulo 3).

**Contenido del Módulo 3:**  
Este módulo ahora incluye todos los videos que antes estaban en la evaluación de recuperación del Módulo 3 y la evaluación offline del Módulo 4.

**Actualización del Cuaderno de Evaluación:**  
El cuaderno para la evaluación offline ha sido actualizado. Antes usaba Elastic Search y ahora utiliza Quadrant para la búsqueda vectorial.

**Disponibilidad del Cuaderno Actualizado:**  
Se ha proporcionado una versión actualizada del cuaderno para que puedas seguir el video sin necesidad de una base de datos de búsqueda vectorial externa.

**Cambios en el Video 3.4:**  
El contenido del video 3.4, que trataba sobre la evaluación de la recuperación vectorial para Elastic Search, ahora será parte de la tarea, donde evaluarás MinSearch y Quadrant.

### 🎥 **LLM Zoomcamp 3.1 - Evaluation Metrics for Retrieval**

**Importancia de la Evaluación:**  
Es fundamental evaluar los resultados de búsqueda para optimizar cómo se almacenan y recuperan los datos en un sistema RAG (Generación Aumentada por Recuperación). El rendimiento ideal depende de los datos y los requisitos específicos.

**Datos de Referencia (Ground Truth or Gold standard data or Generating ground truth with LLM):**  
Para evaluar un sistema, se necesita un conjunto de datos de referencia, que contiene consultas y los documentos relevantes para cada una. Si no se dispone de datos de producción, se puede generar este conjunto de datos usando un modelo de lenguaje grande (LLM).

**Optimización de la Búsqueda:**  
Se pueden ajustar varios parámetros en la búsqueda, como el tipo de búsqueda y los campos incluidos, para mejorar los resultados. La evaluación ayuda a determinar la mejor combinación de estos parámetros.

**Métricas de Ranking Comunes:**  
Se mencionaron varias métricas para evaluar el ranking, entre ellas:

- Precisión en k (P@k)  
- Recall  
- Mean Average Precision (MAP)  
- Normalized Discounted Cumulative Gain (NDCG)  
- Mean Reciprocal Rank (MRR)  
- F1 Score  
- Area Under the ROC curve (AUC-ROC)  
- Mean Rank (MR)  
- Hit Rate (HR) or Recall at K  
- Expected Reciprocal Rank (ERR)

En el video, se centrarán en el **Hit Rate** y **MRR** para la evaluación.

**Precisión en k (P@k)**  
Mide la proporción de documentos relevantes dentro de los primeros k resultados.

*Fórmula:*  
$P@k = \frac{\text{Número de documentos relevantes en los primeros k resultados}}{k}$

**Explicación Detallada**  
Esta es la métrica más intuitiva. Imagina que buscas algo en Google y solo miras los primeros 10 resultados. La Precisión en 10 (P@10) te dice qué porcentaje de esos 10 enlaces fue realmente útil. No le importa si había más resultados buenos en la página 2; solo se enfoca en la calidad de la primera "vitrina" de resultados que ve el usuario.

**Casos de Uso Comunes**
- Motores de búsqueda web (Google, Bing): La mayoría de los usuarios no pasa de la primera página.
- Búsqueda en e-commerce (Amazon): Si buscas "zapatillas rojas", quieres ver zapatillas rojas inmediatamente.
- Sistemas de recomendación (Netflix, Spotify): Es crucial que las primeras recomendaciones sean atractivas.

**Recall (Exhaustividad o Sensibilidad)**  
Mide la proporción de documentos relevantes que el sistema logró encontrar de entre el total de documentos relevantes que existen.

*Fórmula:*  
$Recall = \frac{\text{Número de documentos relevantes recuperados}}{\text{Número total de documentos relevantes}}$

**Explicación Detallada**  
El Recall se enfoca en la cobertura. No le importa si los resultados buenos están al principio o al final, solo quiere saber si el sistema fue capaz de encontrarlos todos. El objetivo es minimizar los "falsos negativos" o, en otras palabras, no dejar fuera nada importante.

**Casos de Uso Comunes**
- Búsquedas legales o de patentes: Es crítico encontrar todos los documentos pertinentes para un caso. Omitir uno puede tener graves consecuencias.
- Diagnóstico médico: Un médico necesita conocer todos los posibles estudios o condiciones relacionadas con los síntomas de un paciente.
- Sistemas de vigilancia o detección de fraude: Es preferible tener algunas falsas alarmas (baja precisión) a dejar pasar una amenaza real (bajo recall).

**Puntuación F1 (F1 Score)**  
Es la media armónica de la Precisión y el Recall. Busca un equilibrio entre ambos.

*Fórmula:*  
$F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$

**Explicación Detallada**  
La puntuación F1 es el gran pacificador entre Precisión y Recall. Es imposible obtener un F1 alto si una de las dos métricas es muy baja. La media armónica penaliza los valores extremos, por lo que obliga al sistema a ser bueno tanto en no mostrar basura (Precisión) como en no omitir resultados importantes (Recall).

**Casos de Uso Comunes**
- Clasificación de texto y análisis de sentimiento: Cuando el balance entre encontrar todas las menciones (Recall) y que estas sean correctas (Precisión) es igualmente importante.
- Evaluación general de un sistema: Proporciona una única cifra que resume el rendimiento de forma equilibrada.

**Rango Recíproco Medio (Mean Reciprocal Rank - MRR)**  
Evalúa la posición en el ranking del primer documento relevante encontrado.

*Fórmula:*  
$MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$  
donde:  
- $|Q|$ es el número total de búsquedas.  
- $rank_i$ es la posición del primer resultado correcto para la búsqueda $i$.

**Explicación Detallada**  
A esta métrica solo le importa una cosa: ¿qué tan rápido encuentra el usuario la primera respuesta correcta? Si la primera respuesta correcta está en la posición 1, el puntaje es 1. Si está en la posición 2, es 1/2. Si está en la 3, es 1/3, y así sucesivamente. El MRR es el promedio de estos puntajes.

**Casos de Uso Comunes**
- Sistemas de preguntas y respuestas (FAQs): El usuario quiere la respuesta a su pregunta, y la quiere ya.
- Chatbots de soporte: El objetivo es encontrar el artículo de la base de conocimiento que resuelve el problema del usuario lo antes posible.
- Búsquedas de "voy a tener suerte" o autocompletado: El sistema apuesta por un único resultado correcto.

**Ganancia Cumulativa Descontada Normalizada (NDCG)**  
Mide la utilidad (o ganancia) de un documento basándose en su posición en la lista de resultados.

*Fórmula:*  
$NDCG = \frac{DCG}{IDCG}$  
donde:  
- DCG (Ganancia Cumulativa Descontada): Suma la relevancia de cada documento, pero "descuenta" su valor cuanto más abajo esté en la lista.  
- IDCG (DCG Ideal): Es el DCG que se obtendría si los resultados estuvieran perfectamente ordenados por relevancia.

**Explicación Detallada**  
NDCG es la métrica de ranking más completa y usada. Mejora a las demás de dos formas clave:  
1. Reconoce niveles de relevancia: Entiende que un resultado puede ser "perfecto", "bueno" o "regular", asignando más puntos a los mejores.  
2. Penaliza la posición: Da mucho más valor a un resultado "perfecto" en la posición 1 que a ese mismo resultado en la posición 10.

Al normalizarlo (dividiendo por el IDCG), el resultado final es un valor entre 0 y 1, que representa qué tan cerca está el ranking del sistema del ranking perfecto.

**Casos de Uso Comunes**
- Es el estándar de oro para los grandes motores de búsqueda y sistemas de recomendación. Es la mejor forma de medir la calidad general de una lista ordenada de resultados.

**Precisión Media Promedio (Mean Average Precision - MAP)**  
Calcula la precisión promedio para cada búsqueda y luego promedia estos valores entre todas las búsquedas.

*Fórmula:*  
$MAP = \frac{1}{|Q|} \sum_{q \in Q} \text{Average Precision(q)}$

**Explicación Detallada**  
Es una métrica robusta que recompensa poner los documentos relevantes al principio. Para una sola búsqueda, se calcula la precisión en cada punto donde aparece un resultado relevante y se promedia. Luego, el MAP promedia estos valores para muchas búsquedas diferentes. Es menos sofisticada que NDCG porque no maneja niveles de relevancia (un resultado es relevante o no lo es).

**Casos de Uso Comunes**
- Búsqueda de imágenes o documentos: Donde el orden de todos los resultados relevantes es importante.
- Benchmark académico: Fue un estándar en la investigación de recuperación de información antes de que NDCG se popularizara.

**Tasa de Aciertos (Hit Rate - HR)**  
Mide la proporción de búsquedas para las cuales se recuperó al menos un documento relevante en los primeros k resultados.

*Fórmula:*  
$HR@k = \frac{\text{Número de búsquedas con al menos un acierto en el top k}}{|Q|}$

**Explicación Detallada**  
Es una métrica simple de "sí o no". ¿El sistema mostró al menos un resultado útil en la primera página? No le importa si fue uno o cinco, solo que hubo un "acierto".

**Casos de Uso Comunes**
- Sistemas de recomendación: ¿Le recomendamos al usuario al menos una película que le interesó entre las 10 primeras?
- Recuperación de información básica: Para confirmar que el sistema no está completamente perdido y es capaz de dar al menos una respuesta útil.

### 🎥 **LLM Zoomcamp 3.2 - Ground Truth Dataset Generation for Retrieval Evaluation**

**Necesidad de un "Ground Truth"**  
Para evaluar de manera efectiva un sistema de búsqueda, es indispensable tener un conjunto de datos de referencia (también llamado "verdad fundamental" o "estándar de oro"), que contenga preguntas y sus respuestas correctas.

**Estructura del Conjunto de Datos**  
Idealmente, este conjunto de datos consiste en miles de preguntas, y para cada una, se conocen los documentos exactos que contienen la respuesta relevante.

**Simplificación del Problema**  
Para simplificar, el video asume que para cada pregunta solo existe un único documento relevante que la responde.

**Métodos para Crear el Conjunto de Datos**

- **Anotación Humana:**  
  Expertos en la materia revisan y asocian preguntas con sus respuestas. Es el método más preciso, pero también el más lento y costoso.

- **Observación del Usuario:**  
  Se analizan las interacciones reales de los usuarios con el sistema. Esto puede ser evaluado por humanos o, más recientemente, por Modelos de Lenguaje Grandes (LLMs).

**Generación de Datos con un LLM**  
El enfoque principal del video es usar un LLM para generar automáticamente el conjunto de datos.  
El objetivo es crear cinco preguntas de usuario por cada entrada en un documento de preguntas frecuentes (FAQ).

**Creación de un ID de Documento Único**

- **Problema:**  
  Los documentos originales no tienen un identificador único, lo que dificulta el seguimiento si los datos se actualizan.

- **Solución:**  
  Se genera un ID único para cada documento creando un hash (MD5) a partir de su contenido (curso, pregunta y texto). Esto garantiza que el ID no cambie a menos que el contenido lo haga.

**Implementación y Prompt para el LLM**

- Se muestra el código para cargar los documentos y asignarles el ID único.
- Se diseña un prompt detallado para el LLM, instruyéndole que actúe como un estudiante y genere cinco preguntas variadas y completas para cada FAQ, evitando copiar el texto original.

**Ejecución y Resultados**

- Se ejecuta el proceso de generación de preguntas usando el modelo GPT-4.
- Los resultados generados (preguntas asociadas a su ID de documento relevante) se guardan en un archivo.
- Finalmente, este conjunto de datos se convierte en un DataFrame de Pandas y se guarda como un archivo CSV para su uso en la evaluación.
- Se menciona que todo el proceso tuvo un costo aproximado de 4 dólares.

### 🎥 **LLM Zoomcamp 3.3 - Evaluation of Text Retrieval Techniques for RAG out**

**1. El Propósito de la Evaluación**  
El objetivo es medir de forma objetiva qué tan bueno es un sistema de búsqueda. En lugar de confiar en la intuición, usamos métricas para cuantificar el rendimiento. Esto nos permite comparar diferentes sistemas (como Elasticsearch vs. otro motor de búsqueda) o diferentes configuraciones del mismo sistema.

**2. El "Ground Truth" (La Verdad Fundamental)**  

- **Definición:**  
  Es nuestro conjunto de datos de referencia. Contiene pares de "pregunta" y "respuesta correcta". En este contexto, la "respuesta correcta" es el ID del documento que mejor responde a la pregunta.

- **Origen en el Video:**  
  Se creó usando un Modelo de Lenguaje Grande (LLM) que generó 5 preguntas de usuario para cada entrada de una lista de Preguntas Frecuentes (FAQ).

- **Estructura:**  
  Cada fila de este conjunto de datos tiene:
  - `question`: La pregunta que simula la de un usuario.
  - `document`: El ID del documento que se considera la respuesta correcta.
  - `course`: El curso al que pertenece (para filtrar).

**3. Métricas Clave de Evaluación**

Se utilizan dos métricas principales para calificar el sistema:

- **Hit Rate (Tasa de Aciertos) o Recall@k**  
  - **Concepto:**  
    Responde a la pregunta: ¿El documento correcto apareció entre los k primeros resultados? Es una métrica de "éxito o fracaso".

  - **Cómo se usa aquí:**  
    Se revisan los 5 primeros resultados de la búsqueda (`k=5`). Si el ID del documento correcto está en esa lista, es un "acierto" (hit).

  - **Para qué sirve:**  
    Mide la capacidad del sistema para, al menos, encontrar el documento relevante.

- **Mean Reciprocal Rank (MRR) - Rango Recíproco Medio**  
  - **Concepto:**  
    Es más sofisticada que el Hit Rate. No solo le importa si el documento correcto apareció, sino en qué posición lo hizo. Premia a los sistemas que ponen la respuesta correcta más arriba.

  - **Cálculo:**
    - Si la respuesta correcta está en la posición 1, el puntaje es 1.
    - Si está en la posición 2, el puntaje es 1/2 (0.5).
    - Si está en la posición 3, el puntaje es 1/3 (0.33).
    - Si no aparece, el puntaje es 0.
    - El MRR es el promedio de estos puntajes para todas las búsquedas.

  - **Para qué sirve:**  
    Es excelente para medir la eficiencia del sistema desde la perspectiva del usuario. Un MRR alto significa que los usuarios encuentran la respuesta correcta rápidamente.

**4. Implementación Práctica (Elasticsearch)**

- **Indexación:**  
  Antes de buscar, los datos se "indexan". Se definen los tipos de campo:
  - `text`: Para campos con texto libre que necesita ser analizado (como la pregunta y el contenido del documento).
  - `keyword`: Para campos que se usan para filtrado exacto (como el nombre del curso o el ID del documento).

- **Búsqueda (`multi_match`):**  
  La consulta del usuario se busca en múltiples campos a la vez (`question`, `text`, `section`).

- **Boosting:**  
  Se le da más importancia (un `boost` de 3) al campo `question`, asumiendo que si la consulta coincide con una pregunta existente, es más relevante.

- **Filtrado:**  
  Se usa un `filter` para asegurar que los resultados pertenezcan al curso correcto. Esto es más eficiente que incluirlo en la consulta principal.

**5. El Proceso de Evaluación (Paso a Paso)**

1. Cargar el Ground Truth: Se lee el archivo CSV con las preguntas y sus respuestas correctas.  
2. Iterar: Se recorre cada pregunta del Ground Truth.  
3. Buscar: Para cada pregunta, se realiza una búsqueda en el sistema (ej. Elasticsearch).  
4. Evaluar Relevancia: Se comprueba si el `doc_id` correcto está en la lista de resultados devueltos.  
5. Calcular Métricas: Una vez que se tienen los resultados de todas las búsquedas, se calculan el Hit Rate y el MRR totales.

**6. Generalización y Comparación**

- **Función `evaluate`:**  
  Se crea una función genérica que puede evaluar cualquier motor de búsqueda. Esto hace que el código sea modular y reutilizable.

- **Comparación de Sistemas:**  
  El mismo proceso de evaluación se aplica a otro motor de búsqueda (Minsearch) para comparar su rendimiento con Elasticsearch de manera justa y numérica.

**Conclusión clave:**  
La evaluación sistemática con métricas como Hit Rate y MRR es fundamental para entender y mejorar el rendimiento de cualquier sistema de recuperación de información.

### 🎥 **LLM Zoomcamp 3.5 (Former 4.2) - Evaluation and Monitoring in LLMs**

**1. El Flujo de un Sistema RAG (Repaso)** 

Un sistema RAG (Retrieval-Augmented Generation) sigue tres pasos fundamentales para responder una pregunta (`q`):

1.  **Búsqueda (`search`):** Primero, busca en una base de conocimientos (documentos, FAQs, etc.) para encontrar la información más relevante para la pregunta. 
2.  **Construcción del Prompt (`build_prompt`):** Luego, combina la pregunta original del usuario con la información encontrada en la búsqueda para crear un *prompt* detallado. 
3.  **Generación de Respuesta (`llm`):** Finalmente, envía este *prompt* a un Modelo de Lenguaje Grande (LLM) para que genere una respuesta coherente y contextualizada. 

**2. Tipos de Evaluación: ¿Cuándo y Cómo Medir la Calidad?**

Existen dos momentos clave para evaluar el sistema:

* **Evaluación Offline (Antes del Lanzamiento)**
    * **¿Qué es?** Se realiza *antes* de que los usuarios interactúen con el sistema. Es una fase de pruebas en un entorno controlado.
    * **Objetivo:** Asegurarse de que el sistema sea funcional y de alta calidad antes de ponerlo en producción.
    * **Métodos Comunes:**
        * **Uso de un "Ground Truth" Dataset** 
     Se utiliza un conjunto de datos con pares de "pregunta" y "respuesta ideal". Se compara la respuesta generada por nuestro sistema con la "respuesta ideal".
        * **Similitud Coseno** 
     Una técnica matemática para medir qué tan parecida es la respuesta de nuestro sistema a la respuesta ideal del ground truth. Un puntaje alto significa que son muy similares.
        * **LLM como Juez** 
     Se usa otro LLM (como GPT-4) para que actúe como un "juez". Se le da la pregunta, la respuesta de nuestro sistema y, a veces, la respuesta ideal, y se le pide que califique la calidad de nuestra respuesta.

* **Evaluación Online (Después del Lanzamiento)**
    * **¿Qué es?** Se realiza *mientras* los usuarios reales están utilizando el sistema.
    * **Objetivo:** Monitorear el rendimiento en el mundo real y detectar problemas que no se vieron en la fase offline.
    * **Métodos Comunes:**
        * **Feedback del Usuario** 
     La forma más directa. Se recopila la opinión de los usuarios a través de botones de "pulgar arriba/abajo", comentarios, etc.
        * **Pruebas A/B (A/B Testing)** 
     Se divide a los usuarios en grupos. Al grupo A se le muestra la versión actual del sistema y al grupo B una nueva versión con mejoras. Se comparan las métricas para ver qué versión funciona mejor.

**3. Monitoreo Continuo**

* **Concepto:** No es una evaluación puntual, sino la **observación constante** de la salud y el rendimiento del sistema una vez que está en producción.
* **¿Qué se monitorea?**
    * **Salud Técnica:** Métricas como el uso de CPU, la latencia (cuánto tarda en responder), y si hay errores. 

    * **Calidad de las Respuestas:** Se siguen recolectando datos del feedback de los usuarios y se observan las tendencias. Si la calidad de las respuestas empieza a bajar, el monitoreo nos alerta para que podamos investigar y corregir el problema. 

La **evaluación offline** nos ayuda a construir un buen sistema, la **evaluación online** nos confirma si funciona bien con usuarios reales, y el **monitoreo** nos asegura que se mantenga así a lo largo del tiempo.

### 🎥 **LLM Zoomcamp 3.6 (Former 4.3) - Offline RAG Evaluation**

**1. ¿Qué es la Evaluación Offline de RAG?** 🧐

Es el proceso de **medir la calidad** de un sistema completo de Generación Aumentada por Recuperación (RAG) **antes** de que lo usen los usuarios finales. El objetivo es tomar decisiones informadas sobre qué componentes usar (por ejemplo, qué modelo de LLM es mejor o más rentable).

**2. Componentes Clave de un Sistema RAG**

Un sistema RAG tiene tres partes principales que trabajan juntas:

1.  **Búsqueda:** Encuentra y recupera los documentos más relevantes de una base de conocimientos. En el video, se usa **Elasticsearch** con una búsqueda de "vecinos más cercanos" (k-NN) sobre vectores.
2.  **Construcción del Prompt:** Toma la pregunta del usuario y los documentos encontrados para crear una instrucción clara para el LLM.
3.  **LLM (Modelo de Lenguaje Grande):** Genera la respuesta final basándose en la información del prompt.

**3. El Proceso de Evaluación: Paso a Paso**

* **Indexación de Datos** 
    * **Concepto:** Convertir los documentos de texto en vectores numéricos (embeddings) para que la máquina pueda entenderlos y compararlos.
    * **Herramienta Usada:** Un modelo `SentenceTransformer` llamado `multi-qa-MiniLM-L6-cos-v1`, que es bueno para tareas de preguntas y respuestas.
    * **Resultado:** Se crea un índice en Elasticsearch donde cada documento tiene un vector asociado, permitiendo búsquedas semánticas rápidas.

* **Métrica de Evaluación: Similitud del Coseno**  
    * **Definición:** Es una fórmula matemática que mide qué tan "parecidas" son dos cosas (en este caso, dos textos). Se calcula midiendo el ángulo entre sus vectores.
    * **Escala:**
        * Un valor de **1** significa que son idénticos.
        * Un valor de **0** significa que no tienen nada que ver.
    * **Aplicación:** Se compara la respuesta generada por nuestro sistema RAG con la "respuesta ideal" de nuestro conjunto de datos de prueba (ground truth). Un puntaje alto indica que nuestro sistema está generando respuestas de alta calidad.

**4. Caso de Estudio: GPT-4o vs. GPT-3.5 Turbo**

El video realiza una comparación directa para decidir qué modelo es mejor para su sistema RAG.

* **GPT-4o (El Modelo Potente)** 
    * **Costo:** Aproximadamente **$10**.
    * **Tiempo:** **3 horas** para procesar todo el conjunto de datos.

* **GPT-3.5 Turbo (El Modelo Eficiente)** 
    * **Costo:** Solo **$0.79** (¡mucho más barato!).
    * **Tiempo:** **6.5 minutos** (¡mucho más rápido!).
    * **Optimización:** Se usó `multi-threading` (paralelización) para acelerar el proceso, ejecutando múltiples peticiones al mismo tiempo.

**5. Conclusión Principal** 

La **evaluación offline es crucial**. Permite tomar decisiones basadas en datos concretos sobre el **costo**, la **velocidad** y la **calidad**. En este caso, aunque GPT-4o es más avanzado, **GPT-3.5 Turbo demostró ser una alternativa mucho más rentable y rápida**, con una calidad de respuesta comparable para esta tarea específica.

### 🎥 **LLM Zoomcamp 3.7 (Former 4.4) - Offline RAG Evaluation: Cosine Similarity**

**1. Objetivo de la Clase: Evaluar para Decidir**

* **Propósito principal:** Aprender a usar métricas de evaluación *offline* (antes de salir a producción) para tomar una decisión informada sobre qué Modelo de Lenguaje Grande (LLM) es el mejor para nuestro caso de uso.
* **Criterios de decisión:** No solo se busca la mejor calidad, sino también el mejor balance entre **calidad, costo y velocidad**.

**2. Métrica Clave: Similitud Coseno**

* **Concepto:** Es una técnica para medir qué tan "parecidas" son dos piezas de texto (en nuestro caso, dos respuestas). Lo hace convirtiendo el texto en vectores (números) y midiendo el ángulo entre ellos.
* **Escala de Puntuación:**
    * **1:** Los textos son idénticos o semánticamente muy similares.
    * **0:** Los textos no tienen ninguna relación.
    * **En la práctica:** Un valor cercano a 1 es bueno.
* **Proceso de Aplicación (A -> Q -> A')**:
    1.  **A (Respuesta Original):** Se toma la respuesta "perfecta" de nuestro conjunto de datos de referencia (ground truth).
    2.  **Q (Pregunta Sintética):** Se usa un LLM para generar una pregunta que corresponda a esa respuesta.
    3.  **A' (Respuesta Generada):** Se le da la pregunta (Q) al modelo que estamos evaluando (ej. GPT-4o Mini) y se obtiene su respuesta (A').
    4.  **Comparación:** Se calcula la similitud coseno entre la respuesta original (A) y la respuesta generada (A').

**3. Caso de Estudio: Comparando Tres Modelos de OpenAI**

Se evaluaron tres modelos populares para ver cuál se desempeñaba mejor en la tarea.

* **Candidatos:**
    1.  **GPT-4**
    2.  **GPT-3.5 Turbo**
    3.  **GPT-4o Mini** (un modelo más nuevo y económico)

* **Implementación Práctica:**
    1.  Se cargan los datos de referencia y las respuestas previamente generadas por cada modelo.
    2.  Se crea una función `compute_similarity` que automatiza el cálculo de la similitud coseno para cualquier par de respuestas.
    3.  Se aplica esta función a todas las respuestas de cada modelo para obtener una lista de puntuaciones.

* **Análisis de Resultados:**
    * **Puntuaciones Promedio de Similitud:**
        * GPT-4o Mini: 0.683
        * GPT-4: 0.679
        * GPT-3.5 Turbo: 0.657

    * **Observación Clave sobre la Calidad:**
        * Al visualizar las distribuciones de las puntuaciones, se observa que los tres modelos se comportan de manera **muy similar**. No hay un ganador claro solo por la calidad de la respuesta; todos son bastante buenos.

**4. Más Allá de la Calidad: Costo y Velocidad**

Dado que la calidad es similar, los factores decisivos son el costo y la velocidad.

* **Costo:**
    * GPT-4o Mini es **significativamente más barato** que GPT-3.5 y GPT-4.

* **Velocidad:**
    * GPT-4o Mini es **más rápido** que GPT-3.5 Turbo.

* **Consideración Técnica: Límites de Tasa (Rate Limits)**
    * Se encontró un problema práctico con GPT-4o Mini: tiene límites de peticiones por minuto más estrictos.
    * **Solución:** Para procesar grandes volúmenes de datos, es necesario implementar `multi-threading` (procesamiento en paralelo) o espaciar las solicitudes para no exceder el límite.

**5. Conclusión Final de la Clase**

* **El Ganador:** Basado en la evidencia, **GPT-4o Mini es la mejor opción** para este caso de uso.
* **Razonamiento:** Ofrece una calidad de respuesta comparable (o incluso ligeramente superior) a los modelos más caros, pero a una fracción del costo y con mayor velocidad. La gestión de los límites de tasa es un pequeño obstáculo técnico que vale la pena superar por los beneficios obtenidos.
* **Lección Aprendida:** La evaluación no debe basarse solo en una métrica. Es fundamental analizar el panorama completo (calidad, costo, velocidad, limitaciones técnicas) para tomar la mejor decisión de ingeniería.

### 🎥 **LLM Zoomcamp 3.8 (Former 4.5) - Offline RAG Evaluation: LLM as a Judge**

**1. Introducción: ¿Por qué necesitamos un "Juez"?**

* **El Problema:** Ya vimos cómo usar la **similitud coseno** para medir la calidad de las respuestas de nuestro sistema RAG. Sin embargo, este método requiere tener una "respuesta ideal" (ground truth) para comparar. ¿Qué pasa cuando no la tenemos, especialmente en un entorno de producción en vivo?
* **La Solución:** Usar otro Modelo de Lenguaje Grande (LLM) para que actúe como un **juez imparcial**. Le pedimos al LLM que evalúe la calidad de la respuesta generada por nuestro sistema.

**2. Escenarios de Evaluación con un LLM Juez**

Se exploran dos escenarios principales para esta técnica:

* **Escenario 1: Evaluación Offline (con "chuleta")**
    * **Contexto:** Se realiza antes del lanzamiento, en un entorno controlado.
    * **Información disponible para el juez:**
        1.  La **pregunta** del usuario.
        2.  La **respuesta generada** por nuestro sistema RAG.
        3.  La **respuesta original/ideal** (del ground truth).
    * **Tarea del Juez:** Comparar la respuesta generada con la ideal y determinar si es relevante, parcialmente relevante o no relevante.

* **Escenario 2: Evaluación Online (a ciegas)**
    * **Contexto:** Se realiza en tiempo real, con usuarios reales.
    * **Información disponible para el juez:**
        1.  La **pregunta** del usuario.
        2.  La **respuesta generada** por nuestro sistema RAG.
    * **Tarea del Juez:** Evaluar si la respuesta es coherente y relevante para la pregunta, *sin* tener una respuesta ideal con la que comparar. Este es el escenario más realista y útil para el monitoreo continuo.

**3. Creación de los Prompts para el Juez**

Para que el LLM actúe como un buen juez, necesita instrucciones claras. Se crearon dos prompts principales:

* **Prompt 1 (Para Evaluación Offline):**
    * **Instrucción clave:** "Analiza la respuesta generada en relación con la respuesta original y clasifícala como RELEVANTE, PARCIALMENTE RELEVANTE o NO RELEVANTE. Proporciona una explicación".
    * **Resultado en el video:** Todas las respuestas fueron clasificadas como "RELEVANTES", lo que indica que el sistema RAG funciona bien cuando tiene el contexto correcto.

* **Prompt 2 (Para Evaluación Online):**
    * **Instrucción clave:** "Analiza la relevancia de la respuesta generada en relación con la pregunta y clasifícala. Proporciona una explicación".
    * **Resultado en el video:**
        * 129 respuestas "RELEVANTES".
        * 18 respuestas "PARCIALMENTE RELEVANTES".
        * 3 respuestas "NO RELEVANTES".

**4. Análisis de los Resultados y Lecciones Aprendidas**

* **La importancia de los fallos:** Las respuestas clasificadas como "NO RELEVANTES" son las más valiosas. Nos dan pistas sobre dónde está fallando nuestro sistema RAG.
* **Ejemplo de un fallo**:
    * **Pregunta:** "¿Cómo inicio el demonio de Docker en Linux?"
    * **Respuesta del sistema:** Explicó qué es un demonio de Docker pero **no proporcionó los comandos específicos**.
    * **Veredicto del Juez:** "NO RELEVANTE", porque no respondió directamente a la necesidad del usuario.
    * **Diagnóstico:** El problema probablemente estuvo en la etapa de **búsqueda (retrieval)**. El sistema recuperó un documento que hablaba sobre Docker en general, pero no el que contenía los comandos de inicio.
* **Observación técnica:** Es crucial asegurarse de que la salida del LLM juez esté en un formato estructurado (como JSON) para poder procesar y analizar los resultados automáticamente.

**Conclusión Final**

Usar un **LLM como juez** es una técnica poderosa y flexible para evaluar la calidad de los sistemas RAG, especialmente en producción. Nos permite identificar fallos específicos en el flujo (búsqueda, prompt o generación) y obtener información valiosa para mejorar continuamente la experiencia del usuario.